{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"46eb6063b3754f45996a5f3e84817f61":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3eb4a29511184341ac5c514d1e8738c1","IPY_MODEL_7e3d0cc15e4e4ea4a4a5e2400ef68b64","IPY_MODEL_fd71120efd6a4c29ac659d43d5cdb8e9"],"layout":"IPY_MODEL_f0b344279c7e4cb29304132460ba70a1"}},"3eb4a29511184341ac5c514d1e8738c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e666a8b7ebb14b4aa7589f1a1bc37157","placeholder":"‚Äã","style":"IPY_MODEL_b8130811f37a439b95a9b7507145a191","value":"Map:‚Äá100%"}},"7e3d0cc15e4e4ea4a4a5e2400ef68b64":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18aaa42f4a4a4b109b0f58c823ede3e5","max":900,"min":0,"orientation":"horizontal","style":"IPY_MODEL_60a8977d11e1469bacb832507022f35a","value":900}},"fd71120efd6a4c29ac659d43d5cdb8e9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fc4295793354da6a173c6ade75dbbbf","placeholder":"‚Äã","style":"IPY_MODEL_f59386a1cd414a979006522ad13fdac4","value":"‚Äá900/900‚Äá[00:02&lt;00:00,‚Äá448.52‚Äáexamples/s]"}},"f0b344279c7e4cb29304132460ba70a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e666a8b7ebb14b4aa7589f1a1bc37157":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8130811f37a439b95a9b7507145a191":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18aaa42f4a4a4b109b0f58c823ede3e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60a8977d11e1469bacb832507022f35a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0fc4295793354da6a173c6ade75dbbbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f59386a1cd414a979006522ad13fdac4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0873a071fbaa4f6387efd23e7ac39dc5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1531e600986d4473a7360e88359d1a46","IPY_MODEL_18403d589308493db9ba377090a34c44","IPY_MODEL_89c3740c9c174fd0b17a28e9765fc199"],"layout":"IPY_MODEL_4e6a7b499cc747f289e5207d00e30a3d"}},"1531e600986d4473a7360e88359d1a46":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73e8e9a937974d9f9285cf88af5b0120","placeholder":"‚Äã","style":"IPY_MODEL_b9a8e2eecee44423ba75d030f948cf0f","value":"Map:‚Äá100%"}},"18403d589308493db9ba377090a34c44":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c3d5bee60684097b1769b85055d2ba0","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_485bb599b71947e3b4e882d6e2033120","value":100}},"89c3740c9c174fd0b17a28e9765fc199":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20cfd23a98804975bb6dcb25f348946c","placeholder":"‚Äã","style":"IPY_MODEL_5e0e0f1196fc4f3bbed292f0f3de93a0","value":"‚Äá100/100‚Äá[00:00&lt;00:00,‚Äá304.21‚Äáexamples/s]"}},"4e6a7b499cc747f289e5207d00e30a3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73e8e9a937974d9f9285cf88af5b0120":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9a8e2eecee44423ba75d030f948cf0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c3d5bee60684097b1769b85055d2ba0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"485bb599b71947e3b4e882d6e2033120":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"20cfd23a98804975bb6dcb25f348946c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e0e0f1196fc4f3bbed292f0f3de93a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10236750,"sourceType":"datasetVersion","datasetId":6330025},{"sourceId":10237995,"sourceType":"datasetVersion","datasetId":6331033}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# pip install datasets transformers torch\n!pip install sentence-transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rIIh2awoCKMJ","outputId":"32a9dde5-319c-43fc-e58a-6a4abaa97332","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:18:12.284729Z","iopub.execute_input":"2024-12-18T15:18:12.285411Z","iopub.status.idle":"2024-12-18T15:18:22.031119Z","shell.execute_reply.started":"2024-12-18T15:18:12.285376Z","shell.execute_reply":"2024-12-18T15:18:22.029977Z"}},"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.26.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"**Libraries**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nData = \"/kaggle/input/cleaned-pashto-squad/cleaned_SQuAD_Pashto.csv\"\ndata = pd.read_csv(Data)\ntrain_data, test_data = train_test_split(data, test_size=0.15, random_state=40)\ntrain_data.to_csv(\"/kaggle/working/train_data.csv\", index=False)\ntest_data.to_csv(\"/kaggle/working/test_data.csv\", index=False)\nprint(\"Dataset split complete. Train and test datasets saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:18:22.033190Z","iopub.execute_input":"2024-12-18T15:18:22.033481Z","iopub.status.idle":"2024-12-18T15:18:24.660677Z","shell.execute_reply.started":"2024-12-18T15:18:22.033454Z","shell.execute_reply":"2024-12-18T15:18:24.659788Z"}},"outputs":[{"name":"stdout","text":"Dataset split complete. Train and test datasets saved.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments\nimport torch\nimport os\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForQuestionAnswering, \n    TrainingArguments, \n    Trainer\n)\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer\nimport shutil\n","metadata":{"id":"zGepLnupWOEj","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:18:24.661994Z","iopub.execute_input":"2024-12-18T15:18:24.662767Z","iopub.status.idle":"2024-12-18T15:18:43.520510Z","shell.execute_reply.started":"2024-12-18T15:18:24.662722Z","shell.execute_reply":"2024-12-18T15:18:43.519649Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**Data Loader**","metadata":{}},{"cell_type":"code","source":"def load_and_split_data(\n    input_csv, \n    train_ratio=0.7, \n    test_ratio=0.15, \n    random_seed=42\n):\n    df = pd.read_csv(input_csv)\n    df = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n    total_samples = len(df)\n    train_end = int(total_samples * train_ratio)\n    val_end = train_end + int(total_samples * test_ratio)\n    \n    train_df = df.iloc[:train_end]\n    val_df = df.iloc[train_end:val_end]\n    test_df = df.iloc[val_end:]\n    \n    def transform_subset(subset_df):\n        records = []\n        for _, row in tqdm(subset_df.iterrows(), total=len(subset_df), \n                            desc=\"Transforming data\"):\n            context = ' '.join(row['context']) if isinstance(row['context'], list) else row['context']\n            \n            answer_start = context.find(row['answer']) \\\n                if not row['is_impossible'] else -1\n            \n            record = {\n                \"id\": row.get('id', ''),\n                \"title\": row.get('title', ''),\n                \"context\": context,\n                \"question\": row['question'],\n                \"answer\": row['answer'],\n                \"answer_start\": answer_start,\n                \"is_impossible\": row['is_impossible']\n            }\n            records.append(record)\n        return records\n    \n    train_records = transform_subset(train_df)\n    val_records = transform_subset(val_df)\n    test_records = transform_subset(test_df)\n    \n    train_dataset = Dataset.from_pandas(pd.DataFrame(train_records))\n    val_dataset = Dataset.from_pandas(pd.DataFrame(val_records))\n    test_dataset = Dataset.from_pandas(pd.DataFrame(test_records))\n    \n    return DatasetDict({\n        \"train\": train_dataset, \n        \"validation\": val_dataset, \n        \"test\": test_dataset\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:18:43.522922Z","iopub.execute_input":"2024-12-18T15:18:43.523652Z","iopub.status.idle":"2024-12-18T15:18:43.532001Z","shell.execute_reply.started":"2024-12-18T15:18:43.523610Z","shell.execute_reply":"2024-12-18T15:18:43.530977Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"**Pre Processing**","metadata":{}},{"cell_type":"code","source":"def preprocess_function(tokenizer, examples, max_length=384, stride=128):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n    \n    offset_mapping = inputs.pop(\"offset_mapping\")\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    answers = examples[\"answer\"]\n    answer_starts = examples[\"answer_start\"]\n    start_positions = []\n    end_positions = []\n    \n    for i, offset in enumerate(offset_mapping):\n        sample_idx = sample_map[i]\n        answer = answers[sample_idx]\n        start_char = answer_starts[sample_idx]\n        end_char = answer_starts[sample_idx] + len(answer)\n        sequence_ids = inputs.sequence_ids(i)\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n        \n        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n            \n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n    \n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    \n    return inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:18:43.532846Z","iopub.execute_input":"2024-12-18T15:18:43.533394Z","iopub.status.idle":"2024-12-18T15:18:43.557559Z","shell.execute_reply.started":"2024-12-18T15:18:43.533367Z","shell.execute_reply":"2024-12-18T15:18:43.556877Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"**Train**","metadata":{}},{"cell_type":"code","source":"def train_qa_model(\n    data, \n    model_name=\"roberta-base\", \n    num_epochs=5,\n    learning_rate=2e-5,\n    batch_size=32\n):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n    processed_data = data.map(\n        lambda x: preprocess_function(tokenizer, x), \n        batched=True, \n        remove_columns=data[\"train\"].column_names\n    )\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        evaluation_strategy=\"epoch\",\n        learning_rate=learning_rate,\n        per_device_train_batch_size=batch_size,  \n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=num_epochs,\n        weight_decay=0.01,\n        logging_dir=\"./logs\",\n        logging_steps=10,\n        save_steps=500,\n        save_total_limit=2,\n    )\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=processed_data[\"train\"],\n        eval_dataset=processed_data[\"validation\"],\n        tokenizer=tokenizer,\n    )\n    trainer.train()\n    model.save_pretrained(\"./fine_tuned_model\")\n    tokenizer.save_pretrained(\"./fine_tuned_model\")\n    try:\n        shutil.make_archive(\"./fine_tuned_model_archive\", 'zip', \"./fine_tuned_model\")\n        print(f\"Model saved and zipped to ./fine_tuned_model_archive.zip\")\n    except Exception as e:\n        print(f\"Error creating zip archive: {e}\")\n    \n    return trainer, tokenizer, model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:18:43.558449Z","iopub.execute_input":"2024-12-18T15:18:43.558734Z","iopub.status.idle":"2024-12-18T15:18:43.571924Z","shell.execute_reply.started":"2024-12-18T15:18:43.558709Z","shell.execute_reply":"2024-12-18T15:18:43.571181Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"input_csv = \"/kaggle/working/train_data.csv\"\ndataset = load_and_split_data(input_csv)\ntrainer, tokenizer, model = train_qa_model(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:18:43.573012Z","iopub.execute_input":"2024-12-18T15:18:43.573277Z","iopub.status.idle":"2024-12-18T15:59:53.199213Z","shell.execute_reply.started":"2024-12-18T15:18:43.573252Z","shell.execute_reply":"2024-12-18T15:59:53.198346Z"}},"outputs":[{"name":"stderr","text":"Transforming data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2975/2975 [00:00<00:00, 16562.39it/s]\nTransforming data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 637/637 [00:00<00:00, 16692.83it/s]\nTransforming data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 638/638 [00:00<00:00, 16858.60it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f2b24f3d47343efbf6d79b7aa032046"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"395fd7739b154576b277dff06784e9e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8b5c50977324063b8ce59f2aaf6fc0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24849f53679e4644ae6f72f3454e1746"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06af79e96e674f629f4968c3c052c6e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ca0995e3a014d9e83ffa9cdee78cd4d"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2975 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0be62d3a10f497599a0c7ac9579923e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/637 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fea4a7acc27b41d3a4f96a5ab153d113"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/638 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d98a484d8eb84f03823d8bd58f1bd767"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/tmp/ipykernel_23/612374700.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241218_151909-v8m8m87i</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hashmat-lums/huggingface/runs/v8m8m87i' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/hashmat-lums/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hashmat-lums/huggingface' target=\"_blank\">https://wandb.ai/hashmat-lums/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hashmat-lums/huggingface/runs/v8m8m87i' target=\"_blank\">https://wandb.ai/hashmat-lums/huggingface/runs/v8m8m87i</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1840' max='1840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1840/1840 39:59, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.340200</td>\n      <td>0.338157</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.203800</td>\n      <td>0.309232</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.275300</td>\n      <td>0.291818</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.203800</td>\n      <td>0.289530</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.165700</td>\n      <td>0.290670</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Model saved and zipped to ./fine_tuned_model_archive.zip\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"**Inference**","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\n\ndef calculate_exact_match(predicted_answer, actual_answer):\n    if isinstance(predicted_answer, list):\n        predicted_answer = \" \".join(predicted_answer)  \n    if isinstance(actual_answer, list):\n        actual_answer = \" \".join(actual_answer)  \n    return 1 if predicted_answer.strip().lower() == actual_answer.strip().lower() else 0\n\ndef calculate_f1_score(predicted_answer, actual_answer):\n    if isinstance(predicted_answer, list):\n        predicted_answer = \" \".join(predicted_answer)\n    if isinstance(actual_answer, list):\n        actual_answer = \" \".join(actual_answer)\n    \n    pred_tokens = set(predicted_answer.strip().lower().split())\n    actual_tokens = set(actual_answer.strip().lower().split())\n    precision = len(pred_tokens & actual_tokens) / len(pred_tokens) if pred_tokens else 0\n    recall = len(pred_tokens & actual_tokens) / len(actual_tokens) if actual_tokens else 0\n    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\ndef calculate_cosine_similarity(predicted_answer, actual_answer):\n    if isinstance(predicted_answer, list):\n        predicted_answer = \" \".join(predicted_answer)\n    if isinstance(actual_answer, list):\n        actual_answer = \" \".join(actual_answer)\n    \n    # Remove stop words and clean the text\n    def preprocess_text(text):\n        # Remove punctuation and convert to lowercase\n        text = re.sub(r'[^\\w\\s]', '', text.lower())\n        return text\n    \n    # Check if the texts are not empty after preprocessing\n    processed_pred = preprocess_text(predicted_answer)\n    processed_actual = preprocess_text(actual_answer)\n    \n    if not processed_pred or not processed_actual:\n        return 0.0  # Return 0 similarity if either text is empty\n    \n    vectorizer = CountVectorizer(stop_words='english')\n    try:\n        vectorizer.fit([processed_pred, processed_actual])\n        vectorizer_matrix = vectorizer.transform([processed_pred, processed_actual])\n        cos_sim = cosine_similarity(vectorizer_matrix[0:1], vectorizer_matrix[1:2])\n        return cos_sim[0][0]\n    except ValueError:\n        # Fallback if vocabulary creation fails\n        return 0.0\n\ndef display_results_qa_model(tokenizer, model, dataset, num_examples=5):\n    qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n    examples = dataset[\"validation\"].select(range(num_examples))\n    \n    total_exact_match = 0\n    total_f1_score = 0\n    total_cosine_sim = 0\n    \n    print(\"Evaluating on all examples...\\n\")\n    \n    for i, example in enumerate(examples):\n        context = example['context']\n        question = example['question']\n        actual_answer = example['answer'] \n        prediction = qa_pipeline(question=question, context=context)\n        predicted_answer = prediction['answer']\n        predicted_answer = predicted_answer.replace(\"‚Ä¢‚Ä¢\", \"\")\n\n        exact_match = calculate_exact_match(predicted_answer, actual_answer)\n        f1_score = calculate_f1_score(predicted_answer, actual_answer)\n        cosine_sim = calculate_cosine_similarity(predicted_answer, actual_answer)\n        \n        total_exact_match += exact_match\n        total_f1_score += f1_score\n        total_cosine_sim += cosine_sim\n        \n        # print(f\"Example {i+1}:\")\n        # print(f\"Context: {context}\\n\")\n        # print(f\"Question: {question}\")\n        # print(f\"Predicted Answer: {predicted_answer}\")\n        # print(f\"Actual Answer: {actual_answer}\")\n        # print(f\"Score: {prediction['score']:.4f}\")\n        # print(f\"Exact Match: {exact_match}\")\n        # print(f\"F1 Score: {f1_score:.4f}\")\n        # print(f\"Cosine Similarity: {cosine_sim:.4f}\")\n        # print(\"-\" * 80)\n\n    avg_exact_match = total_exact_match / num_examples\n    avg_f1_score = total_f1_score / num_examples\n    avg_cosine_sim = total_cosine_sim / num_examples\n    \n    print(\"Overall Evaluation Results:\")\n    print(f\"Average Exact Match: {avg_exact_match:.4f}\")\n    print(f\"Average F1 Score: {avg_f1_score:.4f}\")\n    print(f\"Average Cosine Similarity: {avg_cosine_sim:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:44:43.967244Z","iopub.execute_input":"2024-12-18T16:44:43.967768Z","iopub.status.idle":"2024-12-18T16:44:43.983150Z","shell.execute_reply.started":"2024-12-18T16:44:43.967732Z","shell.execute_reply":"2024-12-18T16:44:43.982207Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"model_path = \"./fine_tuned_model\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_path)\ninput_csv2 = \"/kaggle/input/cleaned-pashto-squad/cleaned_SQuAD_Pashto.csv\"\ndataset = load_and_split_data(input_csv2)  \ndisplay_results_qa_model(tokenizer, model, dataset, num_examples=750)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:44:45.904683Z","iopub.execute_input":"2024-12-18T16:44:45.905039Z","iopub.status.idle":"2024-12-18T17:04:29.066353Z","shell.execute_reply.started":"2024-12-18T16:44:45.905010Z","shell.execute_reply":"2024-12-18T17:04:29.065251Z"}},"outputs":[{"name":"stderr","text":"Transforming data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3500/3500 [00:00<00:00, 15514.12it/s]\nTransforming data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 750/750 [00:00<00:00, 15681.13it/s]\nTransforming data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 750/750 [00:00<00:00, 16186.64it/s]\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on all examples...\n\nOverall Evaluation Results:\nAverage Exact Match: 0.1880\nAverage F1 Score: 0.3610\nAverage Cosine Similarity: 0.3878\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"**Pertubed Inference**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\n\ndef load_and_clean_dataset(input_csv):\n    data = pd.read_csv(input_csv).dropna()\n    altered_data = data.iloc[:, [2, 3, 4]]\n    def clean_text(text):\n        cleaned_text = re.sub(r'[^\\u0600-\\u06FFa-zA-Z0-9\\s,ÿü€î!\"\\'()-]', '', text)\n        return cleaned_text.strip()\n\n    altered_data.iloc[:, 0] = altered_data.iloc[:, 0].apply(clean_text)\n    altered_data.iloc[:, 1] = altered_data.iloc[:, 1].apply(clean_text)\n    altered_data.iloc[:, 2] = altered_data.iloc[:, 2].apply(clean_text)\n\n    altered_data.columns = ['altered_context', 'altered_question', 'answer'] \n    return altered_data\n\n# data_path = '/kaggle/input/cleaned-pashto-squad/cleaned_SQuAD_Pashto.csv'\n# altered_data = load_and_clean_dataset(data_path)\n# print(\"Cleaned dataset preview:\")\n# print(altered_data.head())\n\n\nfrom transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef calculate_exact_match(predicted_answer, actual_answer):\n    if isinstance(predicted_answer, list):\n        predicted_answer = \" \".join(predicted_answer)\n    if isinstance(actual_answer, list):\n        actual_answer = \" \".join(actual_answer)\n    return 1 if predicted_answer.strip().lower() == actual_answer.strip().lower() else 0\n\ndef calculate_f1_score(predicted_answer, actual_answer):\n    if isinstance(predicted_answer, list):\n        predicted_answer = \" \".join(predicted_answer)\n    if isinstance(actual_answer, list):\n        actual_answer = \" \".join(actual_answer)\n\n    pred_tokens = set(predicted_answer.strip().lower().split())\n    actual_tokens = set(actual_answer.strip().lower().split())\n    precision = len(pred_tokens & actual_tokens) / len(pred_tokens) if pred_tokens else 0\n    recall = len(pred_tokens & actual_tokens) / len(actual_tokens) if actual_tokens else 0\n    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\ndef calculate_cosine_similarity(predicted_answer, actual_answer):\n    if isinstance(predicted_answer, list):\n        predicted_answer = \" \".join(predicted_answer)\n    if isinstance(actual_answer, list):\n        actual_answer = \" \".join(actual_answer)\n\n    vectorizer = CountVectorizer().fit_transform([predicted_answer, actual_answer])\n    cos_sim = cosine_similarity(vectorizer[0:1], vectorizer[1:2])\n    return cos_sim[0][0]\n\ndef evaluate_qa_perturbed(dataset, model, context_col, question_col, answer_col):\n    qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n\n    altered_results = []\n    altered_f1_scores = []\n\n    for index, row in dataset.iterrows():\n        context = row[context_col]\n        question = row[question_col]\n        actual_answer = row[answer_col]\n        prediction = qa_pipeline(question=question, context=context)\n        predicted_answer = prediction['answer']\n        \n        exact_match = calculate_exact_match(predicted_answer, actual_answer)\n        f1_score = calculate_f1_score(predicted_answer, actual_answer)\n        cosine_sim = calculate_cosine_similarity(predicted_answer, actual_answer)\n\n        altered_results.append({\n            'context': context,\n            'question': question,\n            'predicted_answer': predicted_answer,\n            'actual_answer': actual_answer,\n            'exact_match': exact_match,\n            'f1_score': f1_score,\n            'cosine_similarity': cosine_sim\n        })\n\n        altered_f1_scores.append(f1_score)\n    \n    return altered_f1_scores, altered_results\n\nmodel_path = \"./fine_tuned_model\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_path)\naltered_data_path = \"/kaggle/input/pertubated-pashto/pertubated_pashto_translated.csv\"\naltered_data = load_and_clean_dataset(altered_data_path)\naltered_f1_scores, altered_results = evaluate_qa_perturbed(altered_data, model, 'altered_context', 'altered_question', 'answer')\naltered_results_df = pd.DataFrame(altered_results)\naltered_results_df.to_csv(\"altered_results.csv\", index=False, encoding='utf-8')\n\nprint(\"Evaluation results saved to 'altered_results.csv'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T18:04:49.845179Z","iopub.execute_input":"2024-12-18T18:04:49.846005Z"}},"outputs":[{"name":"stdout","text":"Cleaned dataset preview:\n            altered_context altered_question  \\\n0  56d43c5f2ccc5a1400d830aa           ÿ®€åŸàŸÜÿ≥€å   \n1  56bf6e823aeaaa14008c9629           ÿ®€åŸàŸÜÿ≥€å   \n2  56d43f7e2ccc5a1400d830cb           ÿ®€åŸàŸÜÿ≥€å   \n3  56be8a583aeaaa14008c9097           ÿ®€åŸàŸÜÿ≥€å   \n4  56d45abf2ccc5a1400d830e6           ÿ®€åŸàŸÜÿ≥€å   \n\n                                              answer  \n0  'ÿ®€åŸàŸÜÿ≥ ⁄´€åÿ≤€åŸÑ ŸÜŸàŸÑÿ≥-⁄©ÿßÿ±Ÿºÿ± (bi39ÿõjnse bee-YON-say...  \n1  'ÿØ 2005 ⁄©ÿßŸÑ ÿØ ÿ¨ŸàŸÜ ŸæŸá ŸÖ€åÿßÿ¥ÿ™ ⁄©€ê ÿØ ÿ™ŸÇÿØ€åÿ± ŸÖÿßÿ¥ŸàŸÖ ŸÑŸá...  \n2  'ÿØ ⁄ÅÿßŸÜ ÿ™ÿ¥ÿ±€åÿ≠ ÿ¥ŸàŸä ÿπÿµÿ±Ÿä ⁄ö⁄Å€åŸÜŸá ŸÅ€åŸÖ€åŸÜ€åÿ≥Ÿºÿå ÿ®€åŸàŸÜÿ≥Ÿä ÿ≥...  \n3  'ÿ®€åŸàŸÜÿ≥Ÿä ŸæŸá ŸÅÿ±€å⁄âÿ±€å⁄©ÿ≥ÿ®ÿ±⁄´ÿå Ÿº€å⁄©ÿ≥ÿßÿ≥ ⁄©€ê ÿØ ÿ≥€åŸÜŸº ŸÖÿ±€åŸÖ ...  \n4  'ŸæŸá ÿßÿ™Ÿá ⁄©ŸÑŸÜ€ç ⁄©€êÿå ÿ®€åŸàŸÜÿ≥Ÿä ÿßŸà ÿØ ŸÖÿßÿ¥ŸàŸÖÿ™Ÿàÿ® ŸÖŸÑ⁄´ÿ±Ÿä ⁄©€å...  \n","output_type":"stream"},{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"}],"execution_count":null}]}