{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"46eb6063b3754f45996a5f3e84817f61":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3eb4a29511184341ac5c514d1e8738c1","IPY_MODEL_7e3d0cc15e4e4ea4a4a5e2400ef68b64","IPY_MODEL_fd71120efd6a4c29ac659d43d5cdb8e9"],"layout":"IPY_MODEL_f0b344279c7e4cb29304132460ba70a1"}},"3eb4a29511184341ac5c514d1e8738c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e666a8b7ebb14b4aa7589f1a1bc37157","placeholder":"​","style":"IPY_MODEL_b8130811f37a439b95a9b7507145a191","value":"Map: 100%"}},"7e3d0cc15e4e4ea4a4a5e2400ef68b64":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18aaa42f4a4a4b109b0f58c823ede3e5","max":900,"min":0,"orientation":"horizontal","style":"IPY_MODEL_60a8977d11e1469bacb832507022f35a","value":900}},"fd71120efd6a4c29ac659d43d5cdb8e9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fc4295793354da6a173c6ade75dbbbf","placeholder":"​","style":"IPY_MODEL_f59386a1cd414a979006522ad13fdac4","value":" 900/900 [00:02&lt;00:00, 448.52 examples/s]"}},"f0b344279c7e4cb29304132460ba70a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e666a8b7ebb14b4aa7589f1a1bc37157":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8130811f37a439b95a9b7507145a191":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18aaa42f4a4a4b109b0f58c823ede3e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60a8977d11e1469bacb832507022f35a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0fc4295793354da6a173c6ade75dbbbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f59386a1cd414a979006522ad13fdac4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0873a071fbaa4f6387efd23e7ac39dc5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1531e600986d4473a7360e88359d1a46","IPY_MODEL_18403d589308493db9ba377090a34c44","IPY_MODEL_89c3740c9c174fd0b17a28e9765fc199"],"layout":"IPY_MODEL_4e6a7b499cc747f289e5207d00e30a3d"}},"1531e600986d4473a7360e88359d1a46":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73e8e9a937974d9f9285cf88af5b0120","placeholder":"​","style":"IPY_MODEL_b9a8e2eecee44423ba75d030f948cf0f","value":"Map: 100%"}},"18403d589308493db9ba377090a34c44":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c3d5bee60684097b1769b85055d2ba0","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_485bb599b71947e3b4e882d6e2033120","value":100}},"89c3740c9c174fd0b17a28e9765fc199":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20cfd23a98804975bb6dcb25f348946c","placeholder":"​","style":"IPY_MODEL_5e0e0f1196fc4f3bbed292f0f3de93a0","value":" 100/100 [00:00&lt;00:00, 304.21 examples/s]"}},"4e6a7b499cc747f289e5207d00e30a3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73e8e9a937974d9f9285cf88af5b0120":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9a8e2eecee44423ba75d030f948cf0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c3d5bee60684097b1769b85055d2ba0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"485bb599b71947e3b4e882d6e2033120":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"20cfd23a98804975bb6dcb25f348946c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e0e0f1196fc4f3bbed292f0f3de93a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10229201,"sourceType":"datasetVersion","datasetId":6324537},{"sourceId":10229857,"sourceType":"datasetVersion","datasetId":6325018},{"sourceId":10235826,"sourceType":"datasetVersion","datasetId":6329301}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# pip install datasets transformers torch\n!pip install sentence-transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rIIh2awoCKMJ","outputId":"32a9dde5-319c-43fc-e58a-6a4abaa97332","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T11:59:06.626501Z","iopub.execute_input":"2024-12-18T11:59:06.627226Z","iopub.status.idle":"2024-12-18T11:59:15.869043Z","shell.execute_reply.started":"2024-12-18T11:59:06.627195Z","shell.execute_reply":"2024-12-18T11:59:15.868252Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.26.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.1\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"**Libraries**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nData = \"/kaggle/input/cleaned-small-pashto/cleaned_SQuAD_Pashto.csv\"\ndata = pd.read_csv(Data)\ntrain_data, test_data = train_test_split(data, test_size=0.2, random_state=40)\ntrain_data.to_csv(\"/kaggle/working/train_data.csv\", index=False)\ntest_data.to_csv(\"/kaggle/working/test_data.csv\", index=False)\nprint(\"Dataset split complete. Train and test datasets saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:22:11.642676Z","iopub.execute_input":"2024-12-18T13:22:11.643061Z","iopub.status.idle":"2024-12-18T13:22:13.597069Z","shell.execute_reply.started":"2024-12-18T13:22:11.643022Z","shell.execute_reply":"2024-12-18T13:22:13.596185Z"}},"outputs":[{"name":"stdout","text":"Dataset split complete. Train and test datasets saved.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments\nimport torch\nimport os\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForQuestionAnswering, \n    TrainingArguments, \n    Trainer\n)\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer\nimport shutil\n","metadata":{"id":"zGepLnupWOEj","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:21:05.088013Z","iopub.execute_input":"2024-12-18T12:21:05.088364Z","iopub.status.idle":"2024-12-18T12:21:05.094951Z","shell.execute_reply.started":"2024-12-18T12:21:05.088338Z","shell.execute_reply":"2024-12-18T12:21:05.094072Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"**Data Loader**","metadata":{}},{"cell_type":"code","source":"def load_and_split_data(\n    input_csv, \n    train_ratio=0.7, \n    test_ratio=0.15, \n    random_seed=42\n):\n    df = pd.read_csv(input_csv)\n    df = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n    total_samples = len(df)\n    train_end = int(total_samples * train_ratio)\n    val_end = train_end + int(total_samples * test_ratio)\n    \n    train_df = df.iloc[:train_end]\n    val_df = df.iloc[train_end:val_end]\n    test_df = df.iloc[val_end:]\n    \n    def transform_subset(subset_df):\n        records = []\n        for _, row in tqdm(subset_df.iterrows(), total=len(subset_df), \n                            desc=\"Transforming data\"):\n            context = ' '.join(row['context']) if isinstance(row['context'], list) else row['context']\n            \n            answer_start = context.find(row['answer']) \\\n                if not row['is_impossible'] else -1\n            \n            record = {\n                \"id\": row.get('id', ''),\n                \"title\": row.get('title', ''),\n                \"context\": context,\n                \"question\": row['question'],\n                \"answer\": row['answer'],\n                \"answer_start\": answer_start,\n                \"is_impossible\": row['is_impossible']\n            }\n            records.append(record)\n        return records\n    \n    train_records = transform_subset(train_df)\n    val_records = transform_subset(val_df)\n    test_records = transform_subset(test_df)\n    \n    train_dataset = Dataset.from_pandas(pd.DataFrame(train_records))\n    val_dataset = Dataset.from_pandas(pd.DataFrame(val_records))\n    test_dataset = Dataset.from_pandas(pd.DataFrame(test_records))\n    \n    return DatasetDict({\n        \"train\": train_dataset, \n        \"validation\": val_dataset, \n        \"test\": test_dataset\n    })","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Pre Processing**","metadata":{}},{"cell_type":"code","source":"def preprocess_function(tokenizer, examples, max_length=384, stride=128):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n    \n    offset_mapping = inputs.pop(\"offset_mapping\")\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    answers = examples[\"answer\"]\n    answer_starts = examples[\"answer_start\"]\n    start_positions = []\n    end_positions = []\n    \n    for i, offset in enumerate(offset_mapping):\n        sample_idx = sample_map[i]\n        answer = answers[sample_idx]\n        start_char = answer_starts[sample_idx]\n        end_char = answer_starts[sample_idx] + len(answer)\n        sequence_ids = inputs.sequence_ids(i)\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n        \n        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n            \n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n    \n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    \n    return inputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Train**","metadata":{}},{"cell_type":"code","source":"def train_qa_model(\n    data, \n    model_name=\"roberta-base\", \n    num_epochs=8,\n    learning_rate=2e-5,\n    batch_size=32\n):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n    processed_data = data.map(\n        lambda x: preprocess_function(tokenizer, x), \n        batched=True, \n        remove_columns=data[\"train\"].column_names\n    )\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        evaluation_strategy=\"epoch\",\n        learning_rate=learning_rate,\n        per_device_train_batch_size=batch_size,  \n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=num_epochs,\n        weight_decay=0.01,\n        logging_dir=\"./logs\",\n        logging_steps=10,\n        save_steps=500,\n        save_total_limit=2,\n    )\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=processed_data[\"train\"],\n        eval_dataset=processed_data[\"validation\"],\n        tokenizer=tokenizer,\n    )\n    trainer.train()\n    model.save_pretrained(\"./fine_tuned_model\")\n    tokenizer.save_pretrained(\"./fine_tuned_model\")\n    try:\n        shutil.make_archive(\"./fine_tuned_model_archive\", 'zip', \"./fine_tuned_model\")\n        print(f\"Model saved and zipped to ./fine_tuned_model_archive.zip\")\n    except Exception as e:\n        print(f\"Error creating zip archive: {e}\")\n    \n    return trainer, tokenizer, model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T11:51:39.552868Z","iopub.execute_input":"2024-12-18T11:51:39.553187Z","iopub.status.idle":"2024-12-18T11:55:49.050746Z","shell.execute_reply.started":"2024-12-18T11:51:39.553161Z","shell.execute_reply":"2024-12-18T11:55:49.049852Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Transforming data: 100%|██████████| 142/142 [00:00<00:00, 11153.81it/s]\nTransforming data: 100%|██████████| 30/30 [00:00<00:00, 7260.77it/s]\nTransforming data: 100%|██████████| 32/32 [00:00<00:00, 8540.74it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1707edcd358a489ea03b698fc082d605"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fa25dbea2fc49bda4fad2b7edfce121"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ce367c5c8b4408c8fb260fb62334145"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c919cc1b74843efa86aee4dd8d6bbc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cfe6be3ab714f48902fc7d8ed085186"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edc9d1b4c3854e148d0b15cf126f641d"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/142 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c1f083a158c4430b55dc4f4b0ecc61a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/30 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"715bf5086127417a8f537a2c2276c163"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/32 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61b6b76785b64f96a35dbfda9491f03d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/tmp/ipykernel_23/3427154092.py:173: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241218_115200-ez8z0i08</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hashmat-lums/huggingface/runs/ez8z0i08' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/hashmat-lums/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hashmat-lums/huggingface' target=\"_blank\">https://wandb.ai/hashmat-lums/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hashmat-lums/huggingface/runs/ez8z0i08' target=\"_blank\">https://wandb.ai/hashmat-lums/huggingface/runs/ez8z0i08</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='144' max='144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [144/144 02:59, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>4.206400</td>\n      <td>2.315806</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.259200</td>\n      <td>1.874241</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.996900</td>\n      <td>1.541009</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.911300</td>\n      <td>1.170716</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.782300</td>\n      <td>0.918929</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.638400</td>\n      <td>0.809763</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.516200</td>\n      <td>0.829848</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.512300</td>\n      <td>0.795238</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Model saved and zipped to ./fine_tuned_model_archive.zip\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"input_csv = \"/kaggle/working/train_data.csv\"\ndataset = load_and_split_data(input_csv)\ntrainer, tokenizer, model = train_qa_model(dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Inference**","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\n\ndef calculate_exact_match(predicted_answer, actual_answer):\n    if isinstance(predicted_answer, list):\n        predicted_answer = \" \".join(predicted_answer)  \n    if isinstance(actual_answer, list):\n        actual_answer = \" \".join(actual_answer)  \n    return 1 if predicted_answer.strip().lower() == actual_answer.strip().lower() else 0\n\ndef calculate_f1_score(predicted_answer, actual_answer):\n    if isinstance(predicted_answer, list):\n        predicted_answer = \" \".join(predicted_answer)\n    if isinstance(actual_answer, list):\n        actual_answer = \" \".join(actual_answer)\n    \n    pred_tokens = set(predicted_answer.strip().lower().split())\n    actual_tokens = set(actual_answer.strip().lower().split())\n    precision = len(pred_tokens & actual_tokens) / len(pred_tokens) if pred_tokens else 0\n    recall = len(pred_tokens & actual_tokens) / len(actual_tokens) if actual_tokens else 0\n    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\ndef calculate_cosine_similarity(predicted_answer, actual_answer):\n    if isinstance(predicted_answer, list):\n        predicted_answer = \" \".join(predicted_answer)\n    if isinstance(actual_answer, list):\n        actual_answer = \" \".join(actual_answer)\n    \n    vectorizer = CountVectorizer().fit_transform([predicted_answer, actual_answer])\n    cos_sim = cosine_similarity(vectorizer[0:1], vectorizer[1:2])\n    return cos_sim[0][0]\n\ndef display_results_qa_model(tokenizer, model, dataset, num_examples=5):\n    qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n    examples = dataset[\"validation\"].select(range(num_examples))\n    \n    total_exact_match = 0\n    total_f1_score = 0\n    total_cosine_sim = 0\n    \n    print(\"Evaluating on 5 examples...\\n\")\n    \n    for i, example in enumerate(examples):\n        context = example['context']\n        question = example['question']\n        actual_answer = example['answer'] \n        prediction = qa_pipeline(question=question, context=context)\n        predicted_answer = prediction['answer']\n        predicted_answer = predicted_answer.replace(\"••\", \"\")\n\n        exact_match = calculate_exact_match(predicted_answer, actual_answer)\n        f1_score = calculate_f1_score(predicted_answer, actual_answer)\n        cosine_sim = calculate_cosine_similarity(predicted_answer, actual_answer)\n        \n        total_exact_match += exact_match\n        total_f1_score += f1_score\n        total_cosine_sim += cosine_sim\n        \n        print(f\"Example {i+1}:\")\n        print(f\"Context: {context}\\n\")\n        print(f\"Question: {question}\")\n        print(f\"Predicted Answer: {predicted_answer}\")\n        print(f\"Actual Answer: {actual_answer}\")\n        print(f\"Score: {prediction['score']:.4f}\")\n        print(f\"Exact Match: {exact_match}\")\n        print(f\"F1 Score: {f1_score:.4f}\")\n        print(f\"Cosine Similarity: {cosine_sim:.4f}\")\n        print(\"-\" * 80)\n\n    avg_exact_match = total_exact_match / num_examples\n    avg_f1_score = total_f1_score / num_examples\n    avg_cosine_sim = total_cosine_sim / num_examples\n    \n    print(\"Overall Evaluation Results:\")\n    print(f\"Average Exact Match: {avg_exact_match:.4f}\")\n    print(f\"Average F1 Score: {avg_f1_score:.4f}\")\n    print(f\"Average Cosine Similarity: {avg_cosine_sim:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:53:16.130207Z","iopub.execute_input":"2024-12-18T12:53:16.130581Z","iopub.status.idle":"2024-12-18T12:53:20.488686Z","shell.execute_reply.started":"2024-12-18T12:53:16.130548Z","shell.execute_reply":"2024-12-18T12:53:20.487712Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Transforming data: 100%|██████████| 142/142 [00:00<00:00, 6837.70it/s]\nTransforming data: 100%|██████████| 30/30 [00:00<00:00, 6420.51it/s]\nTransforming data: 100%|██████████| 32/32 [00:00<00:00, 6681.82it/s]\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on 5 examples...\n\nExample 1:\nContext: ['د نیویارک د لویو اوسیدونکو ولسوالیو ځانګړتیا اکثرا د ښکلي ••نسوري سټون قطارونو•• او ښارګوټو او شګو ټاټوبو لخوا تعریف شوي چې د 1870 څخه تر 1930 پورې د ګړندۍ پراختیا په جریان کې رامینځته شوي. په مقابل کې ، د نیویارک ښار هم داسې ګاونډیان لري چې لږ کثافت لري. نفوس لرونکی او وړیا استوګنځایونه. په ګاونډیو کې لکه ریورډیل (برونکس کې) ، ډیټماس پارک (په بروکلین کې) ، او ډګلاسټن (کوینز کې) ، لوی واحد کورنۍ کورونه په مختلف معماري سټایلونو کې عام دي لکه د ټوډور بیا ژوندی کول او ویکټورین.']\n\nQuestion: کوم ډول د کور جوړښت د NYC ډیری لوی استوګنې ولسوالۍ جوړوي؟\nPredicted Answer: نسوري\nActual Answer: ['د نسواري ډبرو قطارونه']\nScore: 0.0004\nExact Match: 0\nF1 Score: 0.0000\nCosine Similarity: 0.0000\n--------------------------------------------------------------------------------\nExample 2:\nContext: ['د لومړنیو متنونو شواهد ښیي چې سدھارتا ګوتم په یوه ټولنه کې زیږیدلی و چې په جغرافیایي او کلتوري لحاظ د هند شمال ختیځ نیمه وچه کې په پنځمه پیړۍ کې زیږیدلی و. دا یا یو کوچنی جمهوریت و، په کوم حالت کې د هغه پلار ••انتخاب شوی مشر•• و، یا یو oligarchy، په دې حالت کې د هغه پلار یو oligarch و.']\n\nQuestion: که سدھارتا په یوه کوچني جمهوریت کې ژوند کاوه، پلار به یې څه و؟\nPredicted Answer: مشر\nActual Answer: ['مشر وټاکل شو']\nScore: 0.0010\nExact Match: 0\nF1 Score: 0.5000\nCosine Similarity: 0.5774\n--------------------------------------------------------------------------------\nExample 3:\nContext: ['د فبروري په 8، 2015، د 57 کلني ګرامي ایوارډونو کې، ویسټ سټیج ته ولاړ کله چې بیک د کال د البم لپاره د هغه جایزه ومنله او بیا له سټیج څخه لاړ، هرڅوک فکر کوي چې هغه په ټوکه کې و. د جایزې له نندارې وروسته ، ویسټ په یوه مرکه کې وویل چې هغه ټوکه نه کوي او دا چې بیک ته اړتیا لري هنر ته درناوی وکړي ، هغه باید خپله جایزه ••بیونسي•• ته ورکړې وای. د فبروري په 26، 2015، هغه په عامه توګه په ټویټر کې له بیک څخه بخښنه وغوښته.']\n\nQuestion: کني چا ته وویل چې بیک باید خپله جایزه ورته وسپاري؟\nPredicted Answer: بیونسي\nActual Answer: ['بیونس']\nScore: 0.0010\nExact Match: 0\nF1 Score: 0.0000\nCosine Similarity: 0.0000\n--------------------------------------------------------------------------------\nExample 4:\nContext: ['پورتني نور بیا د وجود په 31 الوتکو ویشل شوي دي. [ویب 4] په ځینو لوړو اسمانونو کې بیا زیږون، چې د SHuddhavāsa Worlds یا Pure Abodes په نوم پیژندل کیږي، یوازې د تکړه بودایی متخصصینو لخوا ترلاسه کیدی شي چې د انګامیس (نه راستنیدونکي) په نوم پیژندل کیږي. په اروپیاداتو (••بې شکله حقیقتونو••) کې بیا زیږون یوازې هغه څوک ترلاسه کولی شي چې په اروپجناس کې مراقبت کولی شي ، د مراقبت ترټولو لوړ څیز.']\n\nQuestion: اروپیاداتو څه معنی لري؟\nPredicted Answer: بې\nActual Answer: ['بې شکله سیمې']\nScore: 0.0002\nExact Match: 0\nF1 Score: 0.5000\nCosine Similarity: 0.5774\n--------------------------------------------------------------------------------\nExample 5:\nContext: ['چوپین د نڅا مشهور ډولونه هم د ډیری میلوډي او بیان سره سره ورکړل. د چوپین مزورکاس، په داسې حال کې چې د پولنډي دودیزې نڅا (مزوریک) څخه سرچینه اخلي، د دودیز ډول څخه توپیر لري چې دوی د نڅا د تالار پرځای د کنسرټ تالار لپاره لیکل شوي؛ دا چوپین و چې مازورکا یې د اروپا میوزیک نقشه کې واچوله. د اوو پولونیزونو لړۍ د هغه په ژوند کې خپره شوې (بل ••نهه•• وروسته له هغه خپاره شوي) ، د Op سره پیل کیږي. 26 جوړه (په 1836 کې خپره شوې)، په بڼه کې د موسیقۍ لپاره یو نوی معیار ترتیب کړ. د هغه والټزونه هم په ځانګړي ډول د بال روم پرځای د سیلون تلاوت لپاره لیکل شوي او په مکرر ډول د دوی د نڅا پوړ انډولونو په پرتله خورا ګړندي ټیمپو کې دي.']\n\nQuestion: د چوپین له مړینې وروسته څو پولونیز خپاره شول؟\nPredicted Answer: نهه\nActual Answer: ['نهه']\nScore: 0.0012\nExact Match: 1\nF1 Score: 1.0000\nCosine Similarity: 1.0000\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"model_path = \"./fine_tuned_model\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_path)\ninput_csv2 = \"/kaggle/working/test_data.csv\"\ndataset = load_and_split_data(input_csv2)  \ndisplay_results_qa_model(tokenizer, model, dataset, num_examples=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}