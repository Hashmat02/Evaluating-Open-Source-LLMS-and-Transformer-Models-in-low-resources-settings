{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"46eb6063b3754f45996a5f3e84817f61":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3eb4a29511184341ac5c514d1e8738c1","IPY_MODEL_7e3d0cc15e4e4ea4a4a5e2400ef68b64","IPY_MODEL_fd71120efd6a4c29ac659d43d5cdb8e9"],"layout":"IPY_MODEL_f0b344279c7e4cb29304132460ba70a1"}},"3eb4a29511184341ac5c514d1e8738c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e666a8b7ebb14b4aa7589f1a1bc37157","placeholder":"â€‹","style":"IPY_MODEL_b8130811f37a439b95a9b7507145a191","value":"Map:â€‡100%"}},"7e3d0cc15e4e4ea4a4a5e2400ef68b64":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18aaa42f4a4a4b109b0f58c823ede3e5","max":900,"min":0,"orientation":"horizontal","style":"IPY_MODEL_60a8977d11e1469bacb832507022f35a","value":900}},"fd71120efd6a4c29ac659d43d5cdb8e9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fc4295793354da6a173c6ade75dbbbf","placeholder":"â€‹","style":"IPY_MODEL_f59386a1cd414a979006522ad13fdac4","value":"â€‡900/900â€‡[00:02&lt;00:00,â€‡448.52â€‡examples/s]"}},"f0b344279c7e4cb29304132460ba70a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e666a8b7ebb14b4aa7589f1a1bc37157":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8130811f37a439b95a9b7507145a191":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18aaa42f4a4a4b109b0f58c823ede3e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60a8977d11e1469bacb832507022f35a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0fc4295793354da6a173c6ade75dbbbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f59386a1cd414a979006522ad13fdac4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0873a071fbaa4f6387efd23e7ac39dc5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1531e600986d4473a7360e88359d1a46","IPY_MODEL_18403d589308493db9ba377090a34c44","IPY_MODEL_89c3740c9c174fd0b17a28e9765fc199"],"layout":"IPY_MODEL_4e6a7b499cc747f289e5207d00e30a3d"}},"1531e600986d4473a7360e88359d1a46":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73e8e9a937974d9f9285cf88af5b0120","placeholder":"â€‹","style":"IPY_MODEL_b9a8e2eecee44423ba75d030f948cf0f","value":"Map:â€‡100%"}},"18403d589308493db9ba377090a34c44":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c3d5bee60684097b1769b85055d2ba0","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_485bb599b71947e3b4e882d6e2033120","value":100}},"89c3740c9c174fd0b17a28e9765fc199":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20cfd23a98804975bb6dcb25f348946c","placeholder":"â€‹","style":"IPY_MODEL_5e0e0f1196fc4f3bbed292f0f3de93a0","value":"â€‡100/100â€‡[00:00&lt;00:00,â€‡304.21â€‡examples/s]"}},"4e6a7b499cc747f289e5207d00e30a3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73e8e9a937974d9f9285cf88af5b0120":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9a8e2eecee44423ba75d030f948cf0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c3d5bee60684097b1769b85055d2ba0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"485bb599b71947e3b4e882d6e2033120":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"20cfd23a98804975bb6dcb25f348946c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e0e0f1196fc4f3bbed292f0f3de93a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10229201,"sourceType":"datasetVersion","datasetId":6324537},{"sourceId":10229857,"sourceType":"datasetVersion","datasetId":6325018},{"sourceId":10235826,"sourceType":"datasetVersion","datasetId":6329301}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# pip install datasets transformers torch\n!pip install sentence-transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rIIh2awoCKMJ","outputId":"32a9dde5-319c-43fc-e58a-6a4abaa97332","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T11:59:06.626501Z","iopub.execute_input":"2024-12-18T11:59:06.627226Z","iopub.status.idle":"2024-12-18T11:59:15.869043Z","shell.execute_reply.started":"2024-12-18T11:59:06.627195Z","shell.execute_reply":"2024-12-18T11:59:15.868252Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.26.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.1\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"**Libraries**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nData = \"/kaggle/input/cleaned-small-pashto/cleaned_SQuAD_Pashto.csv\"\ndata = pd.read_csv(Data)\ntrain_data, test_data = train_test_split(data, test_size=0.2, random_state=40)\ntrain_data.to_csv(\"/kaggle/working/train_data.csv\", index=False)\ntest_data.to_csv(\"/kaggle/working/test_data.csv\", index=False)\nprint(\"Dataset split complete. Train and test datasets saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:22:11.642676Z","iopub.execute_input":"2024-12-18T13:22:11.643061Z","iopub.status.idle":"2024-12-18T13:22:13.597069Z","shell.execute_reply.started":"2024-12-18T13:22:11.643022Z","shell.execute_reply":"2024-12-18T13:22:13.596185Z"}},"outputs":[{"name":"stdout","text":"Dataset split complete. Train and test datasets saved.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments\nimport torch\nimport os\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForQuestionAnswering, \n    TrainingArguments, \n    Trainer\n)\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer\nimport shutil\n","metadata":{"id":"zGepLnupWOEj","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:21:05.088013Z","iopub.execute_input":"2024-12-18T12:21:05.088364Z","iopub.status.idle":"2024-12-18T12:21:05.094951Z","shell.execute_reply.started":"2024-12-18T12:21:05.088338Z","shell.execute_reply":"2024-12-18T12:21:05.094072Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"**Data Loader**","metadata":{}},{"cell_type":"code","source":"def load_and_split_data(\n    input_csv, \n    train_ratio=0.7, \n    test_ratio=0.15, \n    random_seed=42\n):\n    df = pd.read_csv(input_csv)\n    df = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n    total_samples = len(df)\n    train_end = int(total_samples * train_ratio)\n    val_end = train_end + int(total_samples * test_ratio)\n    \n    train_df = df.iloc[:train_end]\n    val_df = df.iloc[train_end:val_end]\n    test_df = df.iloc[val_end:]\n    \n    def transform_subset(subset_df):\n        records = []\n        for _, row in tqdm(subset_df.iterrows(), total=len(subset_df), \n                            desc=\"Transforming data\"):\n            context = ' '.join(row['context']) if isinstance(row['context'], list) else row['context']\n            \n            answer_start = context.find(row['answer']) \\\n                if not row['is_impossible'] else -1\n            \n            record = {\n                \"id\": row.get('id', ''),\n                \"title\": row.get('title', ''),\n                \"context\": context,\n                \"question\": row['question'],\n                \"answer\": row['answer'],\n                \"answer_start\": answer_start,\n                \"is_impossible\": row['is_impossible']\n            }\n            records.append(record)\n        return records\n    \n    train_records = transform_subset(train_df)\n    val_records = transform_subset(val_df)\n    test_records = transform_subset(test_df)\n    \n    train_dataset = Dataset.from_pandas(pd.DataFrame(train_records))\n    val_dataset = Dataset.from_pandas(pd.DataFrame(val_records))\n    test_dataset = Dataset.from_pandas(pd.DataFrame(test_records))\n    \n    return DatasetDict({\n        \"train\": train_dataset, \n        \"validation\": val_dataset, \n        \"test\": test_dataset\n    })","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Pre Processing**","metadata":{}},{"cell_type":"code","source":"def preprocess_function(tokenizer, examples, max_length=384, stride=128):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n    \n    offset_mapping = inputs.pop(\"offset_mapping\")\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    answers = examples[\"answer\"]\n    answer_starts = examples[\"answer_start\"]\n    start_positions = []\n    end_positions = []\n    \n    for i, offset in enumerate(offset_mapping):\n        sample_idx = sample_map[i]\n        answer = answers[sample_idx]\n        start_char = answer_starts[sample_idx]\n        end_char = answer_starts[sample_idx] + len(answer)\n        sequence_ids = inputs.sequence_ids(i)\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n        \n        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n            \n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n    \n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    \n    return inputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Train**","metadata":{}},{"cell_type":"code","source":"def train_qa_model(\n    data, \n    model_name=\"roberta-base\", \n    num_epochs=8,\n    learning_rate=2e-5,\n    batch_size=32\n):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n    processed_data = data.map(\n        lambda x: preprocess_function(tokenizer, x), \n        batched=True, \n        remove_columns=data[\"train\"].column_names\n    )\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        evaluation_strategy=\"epoch\",\n        learning_rate=learning_rate,\n        per_device_train_batch_size=batch_size,  \n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=num_epochs,\n        weight_decay=0.01,\n        logging_dir=\"./logs\",\n        logging_steps=10,\n        save_steps=500,\n        save_total_limit=2,\n    )\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=processed_data[\"train\"],\n        eval_dataset=processed_data[\"validation\"],\n        tokenizer=tokenizer,\n    )\n    trainer.train()\n    model.save_pretrained(\"./fine_tuned_model\")\n    tokenizer.save_pretrained(\"./fine_tuned_model\")\n    try:\n        shutil.make_archive(\"./fine_tuned_model_archive\", 'zip', \"./fine_tuned_model\")\n        print(f\"Model saved and zipped to ./fine_tuned_model_archive.zip\")\n    except Exception as e:\n        print(f\"Error creating zip archive: {e}\")\n    \n    return trainer, tokenizer, model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T11:51:39.552868Z","iopub.execute_input":"2024-12-18T11:51:39.553187Z","iopub.status.idle":"2024-12-18T11:55:49.050746Z","shell.execute_reply.started":"2024-12-18T11:51:39.553161Z","shell.execute_reply":"2024-12-18T11:55:49.049852Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Transforming data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [00:00<00:00, 11153.81it/s]\nTransforming data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 7260.77it/s]\nTransforming data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 8540.74it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1707edcd358a489ea03b698fc082d605"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fa25dbea2fc49bda4fad2b7edfce121"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ce367c5c8b4408c8fb260fb62334145"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c919cc1b74843efa86aee4dd8d6bbc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cfe6be3ab714f48902fc7d8ed085186"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edc9d1b4c3854e148d0b15cf126f641d"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/142 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c1f083a158c4430b55dc4f4b0ecc61a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/30 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"715bf5086127417a8f537a2c2276c163"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/32 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61b6b76785b64f96a35dbfda9491f03d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/tmp/ipykernel_23/3427154092.py:173: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241218_115200-ez8z0i08</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hashmat-lums/huggingface/runs/ez8z0i08' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/hashmat-lums/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hashmat-lums/huggingface' target=\"_blank\">https://wandb.ai/hashmat-lums/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hashmat-lums/huggingface/runs/ez8z0i08' target=\"_blank\">https://wandb.ai/hashmat-lums/huggingface/runs/ez8z0i08</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='144' max='144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [144/144 02:59, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>4.206400</td>\n      <td>2.315806</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.259200</td>\n      <td>1.874241</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.996900</td>\n      <td>1.541009</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.911300</td>\n      <td>1.170716</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.782300</td>\n      <td>0.918929</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.638400</td>\n      <td>0.809763</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.516200</td>\n      <td>0.829848</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.512300</td>\n      <td>0.795238</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Model saved and zipped to ./fine_tuned_model_archive.zip\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"input_csv = \"/kaggle/working/train_data.csv\"\ndataset = load_and_split_data(input_csv)\ntrainer, tokenizer, model = train_qa_model(dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Inference**","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\n\ndef calculate_exact_match(predicted_answer, actual_answer):\n    if isinstance(predicted_answer, list):\n        predicted_answer = \" \".join(predicted_answer)  \n    if isinstance(actual_answer, list):\n        actual_answer = \" \".join(actual_answer)  \n    return 1 if predicted_answer.strip().lower() == actual_answer.strip().lower() else 0\n\ndef calculate_f1_score(predicted_answer, actual_answer):\n    if isinstance(predicted_answer, list):\n        predicted_answer = \" \".join(predicted_answer)\n    if isinstance(actual_answer, list):\n        actual_answer = \" \".join(actual_answer)\n    \n    pred_tokens = set(predicted_answer.strip().lower().split())\n    actual_tokens = set(actual_answer.strip().lower().split())\n    precision = len(pred_tokens & actual_tokens) / len(pred_tokens) if pred_tokens else 0\n    recall = len(pred_tokens & actual_tokens) / len(actual_tokens) if actual_tokens else 0\n    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\ndef calculate_cosine_similarity(predicted_answer, actual_answer):\n    if isinstance(predicted_answer, list):\n        predicted_answer = \" \".join(predicted_answer)\n    if isinstance(actual_answer, list):\n        actual_answer = \" \".join(actual_answer)\n    \n    vectorizer = CountVectorizer().fit_transform([predicted_answer, actual_answer])\n    cos_sim = cosine_similarity(vectorizer[0:1], vectorizer[1:2])\n    return cos_sim[0][0]\n\ndef display_results_qa_model(tokenizer, model, dataset, num_examples=5):\n    qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n    examples = dataset[\"validation\"].select(range(num_examples))\n    \n    total_exact_match = 0\n    total_f1_score = 0\n    total_cosine_sim = 0\n    \n    print(\"Evaluating on 5 examples...\\n\")\n    \n    for i, example in enumerate(examples):\n        context = example['context']\n        question = example['question']\n        actual_answer = example['answer'] \n        prediction = qa_pipeline(question=question, context=context)\n        predicted_answer = prediction['answer']\n        predicted_answer = predicted_answer.replace(\"â€¢â€¢\", \"\")\n\n        exact_match = calculate_exact_match(predicted_answer, actual_answer)\n        f1_score = calculate_f1_score(predicted_answer, actual_answer)\n        cosine_sim = calculate_cosine_similarity(predicted_answer, actual_answer)\n        \n        total_exact_match += exact_match\n        total_f1_score += f1_score\n        total_cosine_sim += cosine_sim\n        \n        print(f\"Example {i+1}:\")\n        print(f\"Context: {context}\\n\")\n        print(f\"Question: {question}\")\n        print(f\"Predicted Answer: {predicted_answer}\")\n        print(f\"Actual Answer: {actual_answer}\")\n        print(f\"Score: {prediction['score']:.4f}\")\n        print(f\"Exact Match: {exact_match}\")\n        print(f\"F1 Score: {f1_score:.4f}\")\n        print(f\"Cosine Similarity: {cosine_sim:.4f}\")\n        print(\"-\" * 80)\n\n    avg_exact_match = total_exact_match / num_examples\n    avg_f1_score = total_f1_score / num_examples\n    avg_cosine_sim = total_cosine_sim / num_examples\n    \n    print(\"Overall Evaluation Results:\")\n    print(f\"Average Exact Match: {avg_exact_match:.4f}\")\n    print(f\"Average F1 Score: {avg_f1_score:.4f}\")\n    print(f\"Average Cosine Similarity: {avg_cosine_sim:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:53:16.130207Z","iopub.execute_input":"2024-12-18T12:53:16.130581Z","iopub.status.idle":"2024-12-18T12:53:20.488686Z","shell.execute_reply.started":"2024-12-18T12:53:16.130548Z","shell.execute_reply":"2024-12-18T12:53:20.487712Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Transforming data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [00:00<00:00, 6837.70it/s]\nTransforming data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 6420.51it/s]\nTransforming data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 6681.82it/s]\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on 5 examples...\n\nExample 1:\nContext: ['Ø¯ Ù†ÛŒÙˆÛŒØ§Ø±Ú© Ø¯ Ù„ÙˆÛŒÙˆ Ø§ÙˆØ³ÛŒØ¯ÙˆÙ†Ú©Ùˆ ÙˆÙ„Ø³ÙˆØ§Ù„ÛŒÙˆ ÚØ§Ù†Ú«Ú“ØªÛŒØ§ Ø§Ú©Ø«Ø±Ø§ Ø¯ ÚšÚ©Ù„ÙŠ â€¢â€¢Ù†Ø³ÙˆØ±ÙŠ Ø³Ù¼ÙˆÙ† Ù‚Ø·Ø§Ø±ÙˆÙ†Ùˆâ€¢â€¢ Ø§Ùˆ ÚšØ§Ø±Ú«ÙˆÙ¼Ùˆ Ø§Ùˆ Ø´Ú«Ùˆ Ù¼Ø§Ù¼ÙˆØ¨Ùˆ Ù„Ø®ÙˆØ§ ØªØ¹Ø±ÛŒÙ Ø´ÙˆÙŠ Ú†Û Ø¯ 1870 Ú…Ø®Ù‡ ØªØ± 1930 Ù¾ÙˆØ±Û Ø¯ Ú«Ú“Ù†Ø¯Û Ù¾Ø±Ø§Ø®ØªÛŒØ§ Ù¾Ù‡ Ø¬Ø±ÛŒØ§Ù† Ú©Û Ø±Ø§Ù…ÛŒÙ†ÚØªÙ‡ Ø´ÙˆÙŠ. Ù¾Ù‡ Ù…Ù‚Ø§Ø¨Ù„ Ú©Û ØŒ Ø¯ Ù†ÛŒÙˆÛŒØ§Ø±Ú© ÚšØ§Ø± Ù‡Ù… Ø¯Ø§Ø³Û Ú«Ø§ÙˆÙ†Ú‰ÛŒØ§Ù† Ù„Ø±ÙŠ Ú†Û Ù„Ú– Ú©Ø«Ø§ÙØª Ù„Ø±ÙŠ. Ù†ÙÙˆØ³ Ù„Ø±ÙˆÙ†Ú©ÛŒ Ø§Ùˆ ÙˆÚ“ÛŒØ§ Ø§Ø³ØªÙˆÚ«Ù†ÚØ§ÛŒÙˆÙ†Ù‡. Ù¾Ù‡ Ú«Ø§ÙˆÙ†Ú‰ÛŒÙˆ Ú©Û Ù„Ú©Ù‡ Ø±ÛŒÙˆØ±Ú‰ÛŒÙ„ (Ø¨Ø±ÙˆÙ†Ú©Ø³ Ú©Û) ØŒ Ú‰ÛŒÙ¼Ù…Ø§Ø³ Ù¾Ø§Ø±Ú© (Ù¾Ù‡ Ø¨Ø±ÙˆÚ©Ù„ÛŒÙ† Ú©Û) ØŒ Ø§Ùˆ Ú‰Ú«Ù„Ø§Ø³Ù¼Ù† (Ú©ÙˆÛŒÙ†Ø² Ú©Û) ØŒ Ù„ÙˆÛŒ ÙˆØ§Ø­Ø¯ Ú©ÙˆØ±Ù†Û Ú©ÙˆØ±ÙˆÙ†Ù‡ Ù¾Ù‡ Ù…Ø®ØªÙ„Ù Ù…Ø¹Ù…Ø§Ø±ÙŠ Ø³Ù¼Ø§ÛŒÙ„ÙˆÙ†Ùˆ Ú©Û Ø¹Ø§Ù… Ø¯ÙŠ Ù„Ú©Ù‡ Ø¯ Ù¼ÙˆÚ‰ÙˆØ± Ø¨ÛŒØ§ Ú˜ÙˆÙ†Ø¯ÛŒ Ú©ÙˆÙ„ Ø§Ùˆ ÙˆÛŒÚ©Ù¼ÙˆØ±ÛŒÙ†.']\n\nQuestion: Ú©ÙˆÙ… Ú‰ÙˆÙ„ Ø¯ Ú©ÙˆØ± Ø¬ÙˆÚ“ÚšØª Ø¯ NYC Ú‰ÛŒØ±ÛŒ Ù„ÙˆÛŒ Ø§Ø³ØªÙˆÚ«Ù†Û ÙˆÙ„Ø³ÙˆØ§Ù„Û Ø¬ÙˆÚ“ÙˆÙŠØŸ\nPredicted Answer: Ù†Ø³ÙˆØ±ÙŠ\nActual Answer: ['Ø¯ Ù†Ø³ÙˆØ§Ø±ÙŠ Ú‰Ø¨Ø±Ùˆ Ù‚Ø·Ø§Ø±ÙˆÙ†Ù‡']\nScore: 0.0004\nExact Match: 0\nF1 Score: 0.0000\nCosine Similarity: 0.0000\n--------------------------------------------------------------------------------\nExample 2:\nContext: ['Ø¯ Ù„ÙˆÙ…Ú“Ù†ÛŒÙˆ Ù…ØªÙ†ÙˆÙ†Ùˆ Ø´ÙˆØ§Ù‡Ø¯ ÚšÛŒÙŠ Ú†Û Ø³Ø¯Ú¾Ø§Ø±ØªØ§ Ú«ÙˆØªÙ… Ù¾Ù‡ ÛŒÙˆÙ‡ Ù¼ÙˆÙ„Ù†Ù‡ Ú©Û Ø²ÛŒÚ–ÛŒØ¯Ù„ÛŒ Ùˆ Ú†Û Ù¾Ù‡ Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÙŠ Ø§Ùˆ Ú©Ù„ØªÙˆØ±ÙŠ Ù„Ø­Ø§Ø¸ Ø¯ Ù‡Ù†Ø¯ Ø´Ù…Ø§Ù„ Ø®ØªÛŒÚ Ù†ÛŒÙ…Ù‡ ÙˆÚ†Ù‡ Ú©Û Ù¾Ù‡ Ù¾Ù†ÚÙ…Ù‡ Ù¾ÛŒÚ“Û Ú©Û Ø²ÛŒÚ–ÛŒØ¯Ù„ÛŒ Ùˆ. Ø¯Ø§ ÛŒØ§ ÛŒÙˆ Ú©ÙˆÚ†Ù†ÛŒ Ø¬Ù…Ù‡ÙˆØ±ÛŒØª ÙˆØŒ Ù¾Ù‡ Ú©ÙˆÙ… Ø­Ø§Ù„Øª Ú©Û Ø¯ Ù‡ØºÙ‡ Ù¾Ù„Ø§Ø± â€¢â€¢Ø§Ù†ØªØ®Ø§Ø¨ Ø´ÙˆÛŒ Ù…Ø´Ø±â€¢â€¢ ÙˆØŒ ÛŒØ§ ÛŒÙˆ oligarchyØŒ Ù¾Ù‡ Ø¯Û Ø­Ø§Ù„Øª Ú©Û Ø¯ Ù‡ØºÙ‡ Ù¾Ù„Ø§Ø± ÛŒÙˆ oligarch Ùˆ.']\n\nQuestion: Ú©Ù‡ Ø³Ø¯Ú¾Ø§Ø±ØªØ§ Ù¾Ù‡ ÛŒÙˆÙ‡ Ú©ÙˆÚ†Ù†ÙŠ Ø¬Ù…Ù‡ÙˆØ±ÛŒØª Ú©Û Ú˜ÙˆÙ†Ø¯ Ú©Ø§ÙˆÙ‡ØŒ Ù¾Ù„Ø§Ø± Ø¨Ù‡ ÛŒÛ Ú…Ù‡ ÙˆØŸ\nPredicted Answer: Ù…Ø´Ø±\nActual Answer: ['Ù…Ø´Ø± ÙˆÙ¼Ø§Ú©Ù„ Ø´Ùˆ']\nScore: 0.0010\nExact Match: 0\nF1 Score: 0.5000\nCosine Similarity: 0.5774\n--------------------------------------------------------------------------------\nExample 3:\nContext: ['Ø¯ ÙØ¨Ø±ÙˆØ±ÙŠ Ù¾Ù‡ 8ØŒ 2015ØŒ Ø¯ 57 Ú©Ù„Ù†ÙŠ Ú«Ø±Ø§Ù…ÙŠ Ø§ÛŒÙˆØ§Ø±Ú‰ÙˆÙ†Ùˆ Ú©ÛØŒ ÙˆÛŒØ³Ù¼ Ø³Ù¼ÛŒØ¬ ØªÙ‡ ÙˆÙ„Ø§Ú“ Ú©Ù„Ù‡ Ú†Û Ø¨ÛŒÚ© Ø¯ Ú©Ø§Ù„ Ø¯ Ø§Ù„Ø¨Ù… Ù„Ù¾Ø§Ø±Ù‡ Ø¯ Ù‡ØºÙ‡ Ø¬Ø§ÛŒØ²Ù‡ ÙˆÙ…Ù†Ù„Ù‡ Ø§Ùˆ Ø¨ÛŒØ§ Ù„Ù‡ Ø³Ù¼ÛŒØ¬ Ú…Ø®Ù‡ Ù„Ø§Ú“ØŒ Ù‡Ø±Ú…ÙˆÚ© ÙÚ©Ø± Ú©ÙˆÙŠ Ú†Û Ù‡ØºÙ‡ Ù¾Ù‡ Ù¼ÙˆÚ©Ù‡ Ú©Û Ùˆ. Ø¯ Ø¬Ø§ÛŒØ²Û Ù„Ù‡ Ù†Ù†Ø¯Ø§Ø±Û ÙˆØ±ÙˆØ³ØªÙ‡ ØŒ ÙˆÛŒØ³Ù¼ Ù¾Ù‡ ÛŒÙˆÙ‡ Ù…Ø±Ú©Ù‡ Ú©Û ÙˆÙˆÛŒÙ„ Ú†Û Ù‡ØºÙ‡ Ù¼ÙˆÚ©Ù‡ Ù†Ù‡ Ú©ÙˆÙŠ Ø§Ùˆ Ø¯Ø§ Ú†Û Ø¨ÛŒÚ© ØªÙ‡ Ø§Ú“ØªÛŒØ§ Ù„Ø±ÙŠ Ù‡Ù†Ø± ØªÙ‡ Ø¯Ø±Ù†Ø§ÙˆÛŒ ÙˆÚ©Ú“ÙŠ ØŒ Ù‡ØºÙ‡ Ø¨Ø§ÛŒØ¯ Ø®Ù¾Ù„Ù‡ Ø¬Ø§ÛŒØ²Ù‡ â€¢â€¢Ø¨ÛŒÙˆÙ†Ø³ÙŠâ€¢â€¢ ØªÙ‡ ÙˆØ±Ú©Ú“Û ÙˆØ§ÛŒ. Ø¯ ÙØ¨Ø±ÙˆØ±ÙŠ Ù¾Ù‡ 26ØŒ 2015ØŒ Ù‡ØºÙ‡ Ù¾Ù‡ Ø¹Ø§Ù…Ù‡ ØªÙˆÚ«Ù‡ Ù¾Ù‡ Ù¼ÙˆÛŒÙ¼Ø± Ú©Û Ù„Ù‡ Ø¨ÛŒÚ© Ú…Ø®Ù‡ Ø¨Ø®ÚšÙ†Ù‡ ÙˆØºÙˆÚšØªÙ‡.']\n\nQuestion: Ú©Ù†ÙŠ Ú†Ø§ ØªÙ‡ ÙˆÙˆÛŒÙ„ Ú†Û Ø¨ÛŒÚ© Ø¨Ø§ÛŒØ¯ Ø®Ù¾Ù„Ù‡ Ø¬Ø§ÛŒØ²Ù‡ ÙˆØ±ØªÙ‡ ÙˆØ³Ù¾Ø§Ø±ÙŠØŸ\nPredicted Answer: Ø¨ÛŒÙˆÙ†Ø³ÙŠ\nActual Answer: ['Ø¨ÛŒÙˆÙ†Ø³']\nScore: 0.0010\nExact Match: 0\nF1 Score: 0.0000\nCosine Similarity: 0.0000\n--------------------------------------------------------------------------------\nExample 4:\nContext: ['Ù¾ÙˆØ±ØªÙ†ÙŠ Ù†ÙˆØ± Ø¨ÛŒØ§ Ø¯ ÙˆØ¬ÙˆØ¯ Ù¾Ù‡ 31 Ø§Ù„ÙˆØªÚ©Ùˆ ÙˆÛŒØ´Ù„ Ø´ÙˆÙŠ Ø¯ÙŠ. [ÙˆÛŒØ¨ 4] Ù¾Ù‡ ÚÛŒÙ†Ùˆ Ù„ÙˆÚ“Ùˆ Ø§Ø³Ù…Ø§Ù†ÙˆÙ†Ùˆ Ú©Û Ø¨ÛŒØ§ Ø²ÛŒÚ–ÙˆÙ†ØŒ Ú†Û Ø¯ SHuddhavÄsa Worlds ÛŒØ§ Pure Abodes Ù¾Ù‡ Ù†ÙˆÙ… Ù¾ÛŒÚ˜Ù†Ø¯Ù„ Ú©ÛŒÚ–ÙŠØŒ ÛŒÙˆØ§Ø²Û Ø¯ ØªÚ©Ú“Ù‡ Ø¨ÙˆØ¯Ø§ÛŒÛŒ Ù…ØªØ®ØµØµÛŒÙ†Ùˆ Ù„Ø®ÙˆØ§ ØªØ±Ù„Ø§Ø³Ù‡ Ú©ÛŒØ¯ÛŒ Ø´ÙŠ Ú†Û Ø¯ Ø§Ù†Ú«Ø§Ù…ÛŒØ³ (Ù†Ù‡ Ø±Ø§Ø³ØªÙ†ÛŒØ¯ÙˆÙ†Ú©ÙŠ) Ù¾Ù‡ Ù†ÙˆÙ… Ù¾ÛŒÚ˜Ù†Ø¯Ù„ Ú©ÛŒÚ–ÙŠ. Ù¾Ù‡ Ø§Ø±ÙˆÙ¾ÛŒØ§Ø¯Ø§ØªÙˆ (â€¢â€¢Ø¨Û Ø´Ú©Ù„Ù‡ Ø­Ù‚ÛŒÙ‚ØªÙˆÙ†Ùˆâ€¢â€¢) Ú©Û Ø¨ÛŒØ§ Ø²ÛŒÚ–ÙˆÙ† ÛŒÙˆØ§Ø²Û Ù‡ØºÙ‡ Ú…ÙˆÚ© ØªØ±Ù„Ø§Ø³Ù‡ Ú©ÙˆÙ„ÛŒ Ø´ÙŠ Ú†Û Ù¾Ù‡ Ø§Ø±ÙˆÙ¾Ø¬Ù†Ø§Ø³ Ú©Û Ù…Ø±Ø§Ù‚Ø¨Øª Ú©ÙˆÙ„ÛŒ Ø´ÙŠ ØŒ Ø¯ Ù…Ø±Ø§Ù‚Ø¨Øª ØªØ±Ù¼ÙˆÙ„Ùˆ Ù„ÙˆÚ“ Ú…ÛŒØ².']\n\nQuestion: Ø§Ø±ÙˆÙ¾ÛŒØ§Ø¯Ø§ØªÙˆ Ú…Ù‡ Ù…Ø¹Ù†ÛŒ Ù„Ø±ÙŠØŸ\nPredicted Answer: Ø¨Û\nActual Answer: ['Ø¨Û Ø´Ú©Ù„Ù‡ Ø³ÛŒÙ…Û']\nScore: 0.0002\nExact Match: 0\nF1 Score: 0.5000\nCosine Similarity: 0.5774\n--------------------------------------------------------------------------------\nExample 5:\nContext: ['Ú†ÙˆÙ¾ÛŒÙ† Ø¯ Ù†Ú…Ø§ Ù…Ø´Ù‡ÙˆØ± Ú‰ÙˆÙ„ÙˆÙ†Ù‡ Ù‡Ù… Ø¯ Ú‰ÛŒØ±ÛŒ Ù…ÛŒÙ„ÙˆÚ‰ÙŠ Ø§Ùˆ Ø¨ÛŒØ§Ù† Ø³Ø±Ù‡ Ø³Ø±Ù‡ ÙˆØ±Ú©Ú“Ù„. Ø¯ Ú†ÙˆÙ¾ÛŒÙ† Ù…Ø²ÙˆØ±Ú©Ø§Ø³ØŒ Ù¾Ù‡ Ø¯Ø§Ø³Û Ø­Ø§Ù„ Ú©Û Ú†Û Ø¯ Ù¾ÙˆÙ„Ù†Ú‰ÙŠ Ø¯ÙˆØ¯ÛŒØ²Û Ù†Ú…Ø§ (Ù…Ø²ÙˆØ±ÛŒÚ©) Ú…Ø®Ù‡ Ø³Ø±Ú†ÛŒÙ†Ù‡ Ø§Ø®Ù„ÙŠØŒ Ø¯ Ø¯ÙˆØ¯ÛŒØ² Ú‰ÙˆÙ„ Ú…Ø®Ù‡ ØªÙˆÙ¾ÛŒØ± Ù„Ø±ÙŠ Ú†Û Ø¯ÙˆÛŒ Ø¯ Ù†Ú…Ø§ Ø¯ ØªØ§Ù„Ø§Ø± Ù¾Ø±ÚØ§ÛŒ Ø¯ Ú©Ù†Ø³Ø±Ù¼ ØªØ§Ù„Ø§Ø± Ù„Ù¾Ø§Ø±Ù‡ Ù„ÛŒÚ©Ù„ Ø´ÙˆÙŠØ› Ø¯Ø§ Ú†ÙˆÙ¾ÛŒÙ† Ùˆ Ú†Û Ù…Ø§Ø²ÙˆØ±Ú©Ø§ ÛŒÛ Ø¯ Ø§Ø±ÙˆÙ¾Ø§ Ù…ÛŒÙˆØ²ÛŒÚ© Ù†Ù‚Ø´Ù‡ Ú©Û ÙˆØ§Ú†ÙˆÙ„Ù‡. Ø¯ Ø§ÙˆÙˆ Ù¾ÙˆÙ„ÙˆÙ†ÛŒØ²ÙˆÙ†Ùˆ Ù„Ú“Û Ø¯ Ù‡ØºÙ‡ Ù¾Ù‡ Ú˜ÙˆÙ†Ø¯ Ú©Û Ø®Ù¾Ø±Ù‡ Ø´ÙˆÛ (Ø¨Ù„ â€¢â€¢Ù†Ù‡Ù‡â€¢â€¢ ÙˆØ±ÙˆØ³ØªÙ‡ Ù„Ù‡ Ù‡ØºÙ‡ Ø®Ù¾Ø§Ø±Ù‡ Ø´ÙˆÙŠ) ØŒ Ø¯ Op Ø³Ø±Ù‡ Ù¾ÛŒÙ„ Ú©ÛŒÚ–ÙŠ. 26 Ø¬ÙˆÚ“Ù‡ (Ù¾Ù‡ 1836 Ú©Û Ø®Ù¾Ø±Ù‡ Ø´ÙˆÛ)ØŒ Ù¾Ù‡ Ø¨Ú¼Ù‡ Ú©Û Ø¯ Ù…ÙˆØ³ÛŒÙ‚Û Ù„Ù¾Ø§Ø±Ù‡ ÛŒÙˆ Ù†ÙˆÛŒ Ù…Ø¹ÛŒØ§Ø± ØªØ±ØªÛŒØ¨ Ú©Ú“. Ø¯ Ù‡ØºÙ‡ ÙˆØ§Ù„Ù¼Ø²ÙˆÙ†Ù‡ Ù‡Ù… Ù¾Ù‡ ÚØ§Ù†Ú«Ú“ÙŠ Ú‰ÙˆÙ„ Ø¯ Ø¨Ø§Ù„ Ø±ÙˆÙ… Ù¾Ø±ÚØ§ÛŒ Ø¯ Ø³ÛŒÙ„ÙˆÙ† ØªÙ„Ø§ÙˆØª Ù„Ù¾Ø§Ø±Ù‡ Ù„ÛŒÚ©Ù„ Ø´ÙˆÙŠ Ø§Ùˆ Ù¾Ù‡ Ù…Ú©Ø±Ø± Ú‰ÙˆÙ„ Ø¯ Ø¯ÙˆÛŒ Ø¯ Ù†Ú…Ø§ Ù¾ÙˆÚ“ Ø§Ù†Ú‰ÙˆÙ„ÙˆÙ†Ùˆ Ù¾Ù‡ Ù¾Ø±ØªÙ„Ù‡ Ø®ÙˆØ±Ø§ Ú«Ú“Ù†Ø¯ÙŠ Ù¼ÛŒÙ…Ù¾Ùˆ Ú©Û Ø¯ÙŠ.']\n\nQuestion: Ø¯ Ú†ÙˆÙ¾ÛŒÙ† Ù„Ù‡ Ù…Ú“ÛŒÙ†Û ÙˆØ±ÙˆØ³ØªÙ‡ Ú…Ùˆ Ù¾ÙˆÙ„ÙˆÙ†ÛŒØ² Ø®Ù¾Ø§Ø±Ù‡ Ø´ÙˆÙ„ØŸ\nPredicted Answer: Ù†Ù‡Ù‡\nActual Answer: ['Ù†Ù‡Ù‡']\nScore: 0.0012\nExact Match: 1\nF1 Score: 1.0000\nCosine Similarity: 1.0000\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"model_path = \"./fine_tuned_model\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_path)\ninput_csv2 = \"/kaggle/working/test_data.csv\"\ndataset = load_and_split_data(input_csv2)  \ndisplay_results_qa_model(tokenizer, model, dataset, num_examples=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}