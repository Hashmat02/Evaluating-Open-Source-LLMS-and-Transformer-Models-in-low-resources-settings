{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-18T15:13:07.806893Z",
     "iopub.status.busy": "2024-12-18T15:13:07.806046Z",
     "iopub.status.idle": "2024-12-18T15:13:20.620297Z",
     "shell.execute_reply": "2024-12-18T15:13:20.618983Z",
     "shell.execute_reply.started": "2024-12-18T15:13:07.806855Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Levenshtein\n",
      "  Downloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein)\n",
      "  Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Downloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
      "Successfully installed Levenshtein-0.26.1 rapidfuzz-3.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:14:09.321266Z",
     "iopub.status.busy": "2024-12-18T16:14:09.320073Z",
     "iopub.status.idle": "2024-12-18T16:14:44.342665Z",
     "shell.execute_reply": "2024-12-18T16:14:44.341479Z",
     "shell.execute_reply.started": "2024-12-18T16:14:09.321224Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating original dataset...\n",
      "Accuracy: 68.12% (688/1010)\n",
      "Average F1 Score (Sklearn): 0.68\n",
      "Average Levenshtein Similarity: 0.82\n",
      "Evaluating altered dataset...\n",
      "Accuracy: 46.83% (473/1010)\n",
      "Average F1 Score (Sklearn): 0.47\n",
      "Average Levenshtein Similarity: 0.72\n",
      "Evaluation complete. Results saved to original_results.csv and altered_results.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from Levenshtein import ratio as levenshtein_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  \n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  \n",
    "    cleaned_text = cleaned_text.strip() \n",
    "    return cleaned_text\n",
    "\n",
    "data_path = '/kaggle/input/squad-english/perturbed_dataset_english.csv'\n",
    "data = pd.read_csv(data_path, encoding='utf-8').dropna()\n",
    "\n",
    "data['original_context'] = data['original_context'].apply(clean_text)\n",
    "data['original_question'] = data['original_question'].apply(clean_text)\n",
    "data['answer'] = data['answer'].apply(clean_text)\n",
    "\n",
    "data['altered_context'] = data['altered_context'].apply(clean_text)\n",
    "data['altered_question'] = data['altered_question'].apply(clean_text)\n",
    "\n",
    "original_data = data[['original_context', 'original_question', 'answer']]\n",
    "altered_data = data[['altered_context', 'altered_question', 'answer']]\n",
    "\n",
    "model_name = \"deepset/xlm-roberta-base-squad2\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "def evaluate_qa(dataset, context_column, question_column, answer_column):\n",
    "    results = []\n",
    "    correct_predictions = 0\n",
    "    total_questions = len(dataset)\n",
    "    total_f1 = 0.0\n",
    "    total_levenshtein = 0.0\n",
    "\n",
    "    f1_scores = []\n",
    "    levenshtein_scores = []\n",
    "    \n",
    "    true_answers = []\n",
    "    predicted_answers = []\n",
    "\n",
    "    for idx, row in dataset.iterrows():\n",
    "        context = row[context_column]\n",
    "        question = row[question_column]\n",
    "        true_answer = row[answer_column]\n",
    "        \n",
    "        prediction = qa_pipeline({\"context\": context, \"question\": question})\n",
    "        predicted_answer = prediction.get(\"answer\", \"\")\n",
    "\n",
    "        if true_answer.lower().strip() == predicted_answer.lower().strip():\n",
    "            correct_predictions += 1\n",
    "\n",
    "        true_answers.append(true_answer.lower().strip())\n",
    "        predicted_answers.append(predicted_answer.lower().strip())\n",
    "\n",
    "        levenshtein_sim = levenshtein_similarity(predicted_answer, true_answer)\n",
    "        total_levenshtein += levenshtein_sim\n",
    "        levenshtein_scores.append(levenshtein_sim)\n",
    "\n",
    "        results.append({\n",
    "            \"context\": context,\n",
    "            \"question\": question,\n",
    "            \"true_answer\": true_answer,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"levenshtein_similarity\": levenshtein_sim\n",
    "        })\n",
    "\n",
    "    f1 = f1_score(true_answers, predicted_answers, average='weighted')\n",
    "\n",
    "    accuracy = correct_predictions / total_questions if total_questions > 0 else 0\n",
    "    average_levenshtein = total_levenshtein / total_questions if total_questions > 0 else 0\n",
    "\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}% ({correct_predictions}/{total_questions})\")\n",
    "    print(f\"Average F1 Score (Sklearn): {f1:.2f}\")\n",
    "    print(f\"Average Levenshtein Similarity: {average_levenshtein:.2f}\")\n",
    "    \n",
    "    return f1_scores, levenshtein_scores, results\n",
    "\n",
    "print(\"Evaluating original dataset...\")\n",
    "original_f1_scores, original_levenshtein_scores, original_results = evaluate_qa(original_data, 'original_context', 'original_question', 'answer')\n",
    "\n",
    "print(\"Evaluating altered dataset...\")\n",
    "altered_f1_scores, altered_levenshtein_scores, altered_results = evaluate_qa(altered_data, 'altered_context', 'altered_question', 'answer')\n",
    "\n",
    "original_results_df = pd.DataFrame(original_results)\n",
    "altered_results_df = pd.DataFrame(altered_results)\n",
    "\n",
    "original_results_df.to_csv(\"original_results.csv\", index=False, encoding='utf-8')\n",
    "altered_results_df.to_csv(\"altered_results.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Evaluation complete. Results saved to original_results.csv and altered_results.csv.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Urdu Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:15:02.622252Z",
     "iopub.status.busy": "2024-12-18T16:15:02.621848Z",
     "iopub.status.idle": "2024-12-18T16:15:32.974958Z",
     "shell.execute_reply": "2024-12-18T16:15:32.973619Z",
     "shell.execute_reply.started": "2024-12-18T16:15:02.622215Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating original dataset...\n",
      "Accuracy: 71.74% (528/736)\n",
      "Average F1 Score (Sklearn): 0.72\n",
      "Average Levenshtein Similarity: 0.89\n",
      "Evaluating altered dataset...\n",
      "Accuracy: 14.27% (144/1009)\n",
      "Average F1 Score (Sklearn): 0.15\n",
      "Average Levenshtein Similarity: 0.51\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path = '/kaggle/input/uqa-urdu/pertubated_urdu_translated.csv'\n",
    "data = pd.read_csv(data_path).dropna()\n",
    "\n",
    "altered_data = data.iloc[:, [2, 3, 4]]  \n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^\\u0600-\\u06FFa-zA-Z0-9\\s,؟۔!\"\\'()-]', '', text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "altered_data.iloc[:, 0] = altered_data.iloc[:, 0].apply(clean_text)  \n",
    "altered_data.iloc[:, 1] = altered_data.iloc[:, 1].apply(clean_text)  \n",
    "altered_data.iloc[:, 2] = altered_data.iloc[:, 2].apply(clean_text)  \n",
    "\n",
    "model_name = \"uqa/xlm-roberta-base-UQA-1.0\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "urdu_smallset_path = '/kaggle/input/urdusmall/urdu_smallset.csv'\n",
    "urdu_smallset_data = pd.read_csv(urdu_smallset_path).dropna()\n",
    "\n",
    "original_data = urdu_smallset_data.iloc[:, [2, 3, 5]]  \n",
    "\n",
    "original_data.iloc[:, 0] = original_data.iloc[:, 0].apply(clean_text)  \n",
    "original_data.iloc[:, 1] = original_data.iloc[:, 1].apply(clean_text)  \n",
    "original_data.iloc[:, 2] = original_data.iloc[:, 2]  \n",
    "\n",
    "print(\"Evaluating original dataset...\")\n",
    "original_f1_scores, original_levenshtein_scores, original_results = evaluate_qa(original_data, 0, 1, 2)\n",
    "\n",
    "print(\"Evaluating altered dataset...\")\n",
    "altered_f1_scores, altered_levenshtein_scores, altered_results = evaluate_qa(altered_data, 0,1, 2)\n",
    "\n",
    "original_results_df = pd.DataFrame(original_results)\n",
    "altered_results_df = pd.DataFrame(altered_results)\n",
    "\n",
    "original_results_df.to_csv(\"original_results.csv\", index=False, encoding='utf-8')\n",
    "altered_results_df.to_csv(\"altered_results.csv\", index=False, encoding='utf-8')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6328768,
     "sourceId": 10235123,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6328885,
     "sourceId": 10235281,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6330901,
     "sourceId": 10237819,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
