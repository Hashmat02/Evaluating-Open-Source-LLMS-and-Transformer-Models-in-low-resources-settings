{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10230337,"sourceType":"datasetVersion","datasetId":6325329}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentence-transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:14:43.412588Z","iopub.execute_input":"2024-12-17T22:14:43.412850Z","iopub.status.idle":"2024-12-17T22:14:52.903537Z","shell.execute_reply.started":"2024-12-17T22:14:43.412825Z","shell.execute_reply":"2024-12-17T22:14:52.902424Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.26.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments\nimport torch\nimport os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForQuestionAnswering, \n    TrainingArguments, \n    Trainer\n)\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:14:52.905976Z","iopub.execute_input":"2024-12-17T22:14:52.906361Z","iopub.status.idle":"2024-12-17T22:14:52.982714Z","shell.execute_reply.started":"2024-12-17T22:14:52.906331Z","shell.execute_reply":"2024-12-17T22:14:52.981939Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def load_and_split_data(\n    input_csv, \n    train_ratio=0.7, \n    test_ratio=0.15, \n    random_seed=42\n):\n   \n    df = pd.read_csv(input_csv)\n    df = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n    total_samples = len(df)\n    train_end = int(total_samples * train_ratio)\n    val_end = train_end + int(total_samples * test_ratio)\n    \n    train_df = df.iloc[:train_end]\n    val_df = df.iloc[train_end:val_end]\n    test_df = df.iloc[val_end:]\n    \n    def transform_subset(subset_df):\n        records = []\n        for _, row in tqdm(subset_df.iterrows(), total=len(subset_df), \n                            desc=\"Transforming data\"):\n            answer_start = row['context'].find(row['answer']) \\\n                if not row['is_impossible'] else -1\n            \n            record = {\n                \"id\": row['id'],\n                \"title\": row['title'],\n                \"context\": row['context'],\n                \"question\": row['question'],\n                \"answers\": {\n                    \"text\": [row['answer']] if not row['is_impossible'] else [],\n                    \"answer_start\": [answer_start] if not row['is_impossible'] else []\n                },\n                \"is_impossible\": row['is_impossible'],\n                \"original_answer\": row['answer']\n            }\n            records.append(record)\n        return records\n    \n    train_records = transform_subset(train_df)\n    val_records = transform_subset(val_df)\n    test_records = transform_subset(test_df)\n    \n    train_dataset = Dataset.from_pandas(pd.DataFrame(train_records))\n    val_dataset = Dataset.from_pandas(pd.DataFrame(val_records))\n    test_dataset = Dataset.from_pandas(pd.DataFrame(test_records))\n    \n    return DatasetDict({\n        \"train\": train_dataset, \n        \"validation\": val_dataset, \n        \"test\": test_dataset\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:14:52.983902Z","iopub.execute_input":"2024-12-17T22:14:52.984462Z","iopub.status.idle":"2024-12-17T22:14:52.992536Z","shell.execute_reply.started":"2024-12-17T22:14:52.984422Z","shell.execute_reply":"2024-12-17T22:14:52.991717Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ndef preprocess_function(tokenizer, examples):\n    \n    tokenized = tokenizer(\n        examples[\"question\"], \n        examples[\"context\"], \n        truncation=True, \n        padding=\"max_length\", \n        max_length=512\n    )\n    \n    start_positions = []\n    end_positions = []\n    for answers in examples[\"answers\"]:\n        if answers[\"text\"]:\n            start = answers[\"answer_start\"][0]\n            end = start + len(answers[\"text\"][0])\n        else:\n            start = 0\n            end = 0\n        start_positions.append(start)\n        end_positions.append(end)\n    \n    tokenized[\"start_positions\"] = start_positions\n    tokenized[\"end_positions\"] = end_positions\n    \n    return tokenized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:14:52.993649Z","iopub.execute_input":"2024-12-17T22:14:52.993944Z","iopub.status.idle":"2024-12-17T22:14:53.006053Z","shell.execute_reply.started":"2024-12-17T22:14:52.993919Z","shell.execute_reply":"2024-12-17T22:14:53.005369Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport shutil\nimport os\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments, Trainer\n\ndef train_qa_model(\n    data, \n    model_name=\"roberta-base\", \n    num_epochs=1,\n    learning_rate=2e-5,\n    batch_size=32\n):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n    processed_data = data.map(\n        lambda x: preprocess_function(tokenizer, x), \n        batched=True, \n        remove_columns=data[\"train\"].column_names\n    )\n    \n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        evaluation_strategy=\"epoch\",\n        learning_rate=learning_rate,\n        per_device_train_batch_size=batch_size,  \n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=num_epochs,\n        weight_decay=0.01,\n        logging_dir=\"./logs\",\n        logging_steps=10,\n        save_steps=500,\n        save_total_limit=2,\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=processed_data[\"train\"],\n        eval_dataset=processed_data[\"validation\"],\n        tokenizer=tokenizer,\n    )\n    \n    trainer.train()\n    model.save_pretrained(\"./fine_tuned_model\")\n    tokenizer.save_pretrained(\"./fine_tuned_model\")\n    zip_path = \"/kaggle/working/fine_tuned_model.zip\"\n    shutil.make_archive(\"/kaggle/working/fine_tuned_model\", 'zip', \"./fine_tuned_model\")\n    print(f\"Model saved and zipped to {zip_path}\")\n    return trainer, tokenizer, model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:47:12.619191Z","iopub.execute_input":"2024-12-17T22:47:12.620000Z","iopub.status.idle":"2024-12-17T22:47:12.627533Z","shell.execute_reply.started":"2024-12-17T22:47:12.619967Z","shell.execute_reply":"2024-12-17T22:47:12.626722Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"input_csv = \"/kaggle/input/sindhi/SQuAD_Translated_Sindhi.csv\"\ndata = load_and_split_data(input_csv)\ntrainer, tokenizer, model = train_qa_model(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:47:16.583125Z","iopub.execute_input":"2024-12-17T22:47:16.583483Z","iopub.status.idle":"2024-12-17T22:51:24.105064Z","shell.execute_reply.started":"2024-12-17T22:47:16.583451Z","shell.execute_reply":"2024-12-17T22:51:24.104161Z"}},"outputs":[{"name":"stderr","text":"Transforming data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3500/3500 [00:00<00:00, 15580.95it/s]\nTransforming data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 750/750 [00:00<00:00, 15055.87it/s]\nTransforming data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 750/750 [00:00<00:00, 15144.15it/s]\nSome weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d95e9a4e95c24d5188d0614e39282907"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/750 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20fbfd4421ed4aa5aa605d32eb80af47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/750 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd6cbcf5b84b4dad9fd72089f7807bef"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/tmp/ipykernel_23/1236429608.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [110/110 03:19, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.063600</td>\n      <td>3.170593</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Model saved and zipped to /kaggle/working/fine_tuned_model.zip\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef evaluate_qa_model(\n    model, \n    tokenizer, \n    test_dataset, \n    device=None\n):\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    model = model.to(device)\n    model.eval()\n    \n    exact_match_scores = []\n    cosine_sim_scores = []\n    sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n    \n    with torch.no_grad():\n        for example in tqdm(test_dataset, desc=\"Evaluating\"):\n            inputs = tokenizer(\n                example['question'], \n                example['context'], \n                return_tensors='pt', \n                max_length=512, \n                truncation=True\n            ).to(device)\n            \n            outputs = model(**inputs)\n            start_logits = outputs.start_logits\n            end_logits = outputs.end_logits\n            \n            start_idx = torch.argmax(start_logits).item()\n            end_idx = torch.argmax(end_logits).item()\n            input_ids = inputs['input_ids'][0]\n            predicted_answer_tokens = input_ids[start_idx:end_idx+1]\n            predicted_answer = tokenizer.decode(predicted_answer_tokens).strip()\n            \n            ground_truth = example['original_answer']\n            exact_match_scores.append(int(predicted_answer.lower() == ground_truth.lower()))\n            \n            pred_emb = sentence_model.encode([predicted_answer])\n            gt_emb = sentence_model.encode([ground_truth])\n            cosine_sim_scores.append(cosine_similarity(pred_emb, gt_emb)[0][0])\n    \n    metrics = {\n        \"exact_match_rate\": np.mean(exact_match_scores),\n        \"avg_cosine_similarity\": np.mean(cosine_sim_scores),\n        \"cosine_sim_std\": np.std(cosine_sim_scores)\n    }\n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:43:52.067476Z","iopub.status.idle":"2024-12-17T22:43:52.067795Z","shell.execute_reply.started":"2024-12-17T22:43:52.067614Z","shell.execute_reply":"2024-12-17T22:43:52.067627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_metrics = evaluate_qa_model(model, tokenizer, data[\"test\"])\nprint(\"Evaluation Metrics:\")\nfor key, value in test_metrics.items():\n    print(f\"{key}: {value}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:52:33.122972Z","iopub.execute_input":"2024-12-17T22:52:33.123316Z","iopub.status.idle":"2024-12-17T22:52:33.129443Z","shell.execute_reply.started":"2024-12-17T22:52:33.123286Z","shell.execute_reply":"2024-12-17T22:52:33.128568Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers', 'is_impossible', 'original_answer'],\n        num_rows: 3500\n    })\n    validation: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers', 'is_impossible', 'original_answer'],\n        num_rows: 750\n    })\n    test: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers', 'is_impossible', 'original_answer'],\n        num_rows: 750\n    })\n})\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from transformers import pipeline\nimport numpy as np\n\ndef display_results_qa_model( tokenizer, model, dataset, num_examples=5):\n    \"\"\"\n    Evaluate the fine-tuned model on a few examples.\n    \"\"\"\n    # Load fine-tuned model into a QA pipeline\n    qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n\n    # Select the first `num_examples` from the validation dataset\n    examples = dataset[\"validation\"].select(range(num_examples))\n    \n    print(\"Evaluating on 5 examples...\\n\")\n    \n    for i, example in enumerate(examples):\n        context = example['context']\n        question = example['question']\n        actual_answer = example['original_answer']  # First answer in the list\n        \n        # Get the model's prediction\n        prediction = qa_pipeline(question=question, context=context)\n        predicted_answer = prediction['answer']\n        \n        # Print the details\n        print(f\"Example {i+1}:\")\n        print(f\"Context: {context}\\n\")\n        print(f\"Question: {question}\")\n        print(f\"Predicted Answer: {predicted_answer}\")\n        print(f\"Actual Answer: {actual_answer}\")\n        print(f\"Score: {prediction['score']:.4f}\")\n        print(\"-\" * 80)\n\n# Example usage\n# trainer, tokenizer, model = train_qa_model(data)\ndisplay_results_qa_model( tokenizer, model, data, num_examples=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:53:14.000024Z","iopub.execute_input":"2024-12-17T22:53:14.000392Z","iopub.status.idle":"2024-12-17T22:53:14.295367Z","shell.execute_reply.started":"2024-12-17T22:53:14.000355Z","shell.execute_reply":"2024-12-17T22:53:14.294468Z"}},"outputs":[{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on 5 examples...\n\nExample 1:\nContext: 1973Ø¹ Ø¬ÙŠ Ú†ÙˆÙ†ÚŠÙ† Ú©Ø§Ù† Ù¾ÙˆØ¡Ù Ø³ÙˆØ²Ù„ÙŠÙ†ÚŠ Ø¬Ùˆ Ø¢Ø¦ÙŠÙ† Ø¨Ø§Ø¯Ø´Ø§Ù‡Ù‡ Ø³ÙˆÚ€ÙˆØ²Ø§ II Ù…Ø¹Ø·Ù„ ÚªÙŠÙˆØŒ Ø¬Ù†Ù‡Ù† Ø¨Ø¹Ø¯ Û¾ 1982Ø¹ Û¾ Ù¾Ù†Ù‡Ù†Ø¬ÙŠ ÙˆÙØ§Øª ØªØ§Ø¦ÙŠÙ† ÙØ±Ù…Ø§Ù† Ø°Ø±ÙŠØ¹ÙŠ Ù…Ù„Úª ØªÙŠ Ø­ÚªÙˆÙ…Øª ÚªØ¦ÙŠ. Ø§Ù† ÙˆÙ‚Øª Ø³ÙˆÚ€ÙˆØ²Ø§ II 61 Ø³Ø§Ù„Ù† ØªØ§Ø¦ÙŠÙ† Ø³ÙˆØ²ÙŠÙ„ÙŠÙ†ÚŠ ØªÙŠ Ø­ÚªÙˆÙ…Øª ÚªØ¦ÙŠØŒ Ø¬Ù†Ù‡Ù† Ú©ÙŠØ³ ØªØ§Ø±ÙŠØ® Ø¬Ùˆ Ø³Ú€ Ú©Ø§Ù† ÚŠÚ¯Ù‡Ùˆ Ø­ÚªÙ…Ø±Ø§Ù† Ø¨Ú»Ø§Ø¦ÙŠ Ú‡ÚÙŠÙˆ. Ù‡Ù† Ø¬ÙŠ Ù…ÙˆØª Ø¬ÙŠ Ù¾ÙºÙŠØ§Ù† Ù‡Úª Ø±ÙŠØ¬Ù†Ø³ÙŠ Ø¢Ø¦ÙŠØŒ Ø¬Ù†Ù‡Ù† Û¾ â€Ø±Ø§Ú»ÙŠ Ø±ÙŠØ¬Ù†Ù½ ÚŠÙŠØ²ÙŠÙ„ÙŠ Ø´ÙˆÙ†Ú¯ÙˆÙŠâ€œ 1984Ø¹ ØªØ§Ø¦ÙŠÙ† Ø±ÙŠØ§Ø³Øª Ø¬ÙŠ Ø³Ø±Ø¨Ø±Ø§Ù‡Ù‡ Ø±Ù‡ÙŠØŒ Ø¬ÚÙ‡Ù† Ù‡ÙˆØ¡Ù Ù„ÙŠÚªÙˆÚªÙˆ Ø·Ø±ÙØ§Ù† Ù‡Ù½Ø§Ø¦ÙŠ ÙˆØ¦ÙŠ Û½ Ø§Ù† Ø¬ÙŠ Ø¬Ø§Ø¡Ù ØªÙŠ Ø±Ø§Ú»ÙŠ Ù…Ø§Ø¡Ù Ù†Ù½ÙÙˆÙ…Ø¨ÙŠ Ù½ÙÙˆØ§Ù„Ø§ Ù…Ù‚Ø±Ø± ÚªØ¦ÙŠ ÙˆØ¦ÙŠ. Mswati IIIØŒ Ntfombi Ø¬Ùˆ Ù¾Ù½ØŒ 25 Ø§Ù¾Ø±ÙŠÙ„ 1986 ØªÙŠ Ø³ÙˆØ§Ø²ÙŠÙ„ÙŠÙ†ÚŠ Ø¬ÙŠ Ø¨Ø§Ø¯Ø´Ø§Ù‡Ù‡ Û½ Ø§Ù†Ú¯ÙˆÙ†ÙŠØ§Ù…Ø§ Ø¬ÙŠ Ø­ÙŠØ«ÙŠØª Ø³Ø§Ù† ØªØ§Ø¬ ÚªÙŠÙˆ ÙˆÙŠÙˆ.\n\nQuestion: Ø³ÙˆØ²ÙŠÙ„ÙŠÙ†ÚŠ Û¾ 61 Ø³Ø§Ù„Ù† ØªØ§Ø¦ÙŠÙ† ÚªÙ‡Ú™ÙŠ Ø±ÙŠÙ†Ù½ Ø­ÚªÙˆÙ…Øª ÚªØ¦ÙŠØŸ\nPredicted Answer: .\nActual Answer: Ø±Ø§Ú»ÙŠ Ø±ÙŠØ¬Ù†Ù½ ÚŠÙŠØ²ÙŠÙ„ÙŠ Ø´ÙˆÙ†Ú¯ÙˆÙŠ\nScore: 0.0000\n--------------------------------------------------------------------------------\nExample 2:\nContext: Ø§Ø³Ù„Ø§Ù… ÚÚ©Ú» Ø§ÙˆÚ€Ø± Ø§ÙŠØ´ÙŠØ§ Û¾ Ø³Ú€ Ú©Ø§Ù† ÙˆÚÙŠ Ù¾ÙŠÙ…Ø§Ù†ÙŠ ØªÙŠ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ù¿ÙŠÙ†Ø¯Ú™ Ù…Ø°Ù‡Ø¨ Ø¢Ù‡ÙŠØŒ Ø¬Ù†Ù‡Ù† Û¾ ØªÙ‚Ø±ÙŠØ¨Ù† 240 Ù…Ù„ÙŠÙ† Ù¾ÙŠØ±ÙˆÚªØ§Ø± Ø¢Ù‡Ù† Ø¬ÙŠÚªÙŠ Ø³Ú„ÙŠ Ø¢Ø¨Ø§Ø¯ÙŠ Ø¬Ùˆ 40 Ø³ÙŠÚªÚ™Ùˆ ØªØ§Ø¦ÙŠÙ† ØªØ±Ø¬Ù…Ùˆ ÚªÙ† Ù¿Ø§ØŒ Ø§Ù†ÚŠÙˆÙ†ÙŠØ´ÙŠØ§ØŒ Ø¨Ø±ÙˆÙ†Ø§Ø¦ÙŠØŒ Ù…Ù„Ø§Ø¦ÙŠØ´ÙŠØ§ Û½ ÚØ§Ú©Ú»ÙŠ ÙÙ„Ù¾Ø§Ø¦Ù† Û¾ Ø§ÚªØ«Ø±ÙŠØª Ø§Ù†ÚŠÙˆÙ†ÙŠØ´ÙŠØ§ Ø³Ø§Ù† Ú¯Ú Ø§Ù†ÚŠÙˆÙ†ÙŠØ´ÙŠØ§ Ø¬ÙŠ Ú†ÙˆÚŒØ§Ø±ÙŠ Ø³Ú€ Ú©Ø§Ù† ÙˆÚÙˆ Û½ Ø³Ú€ Ú©Ø§Ù† ÙˆÚŒÙŠÚª Ø¢Ø¨Ø§Ø¯ÙŠ ÙˆØ§Ø±Ùˆ Ù…Ø³Ù„Ù…Ø§Ù† Ù…Ù„Úª Ø¢Ù‡ÙŠ. Ø¯Ù†ÙŠØ§. ÚÚ©Ú» Ø§ÙˆÚ€Ø± Ø§ÙŠØ´ÙŠØ§ Ø¬Ø§ Ù…Ù„Úª ÚªÙŠØªØ±Ù† Ø¦ÙŠ Ù…Ø®ØªÙ„Ù Ù…Ø°Ù‡Ø¨Ù† ØªÙŠ Ø¹Ù…Ù„ ÚªÙ† Ù¿Ø§. Ù¿Ø§Ø¦Ù„ÙŠÙ†ÚŠØŒ ÚªÙ…Ø¨ÙˆÚŠÙŠØ§ØŒ Ù„Ø§ÙˆØ³ØŒ Ø¨Ø±Ù…Ø§ØŒ ÙˆÙŠÙ½Ù†Ø§Ù… Û½ Ø³Ù†Ú¯Ø§Ù¾ÙˆØ± Û¾ Ù»ÚŒÙ…Øª ØºØ§Ù„Ø¨ Ø¢Ù‡ÙŠ. Ø§Ø¨Ù† ÚØ§ÚÙ† Ø¬ÙŠ Ù¾ÙˆÚ„Ø§ Û½ ÚªÙ†ÙÙŠÙˆØ´Ø³Ø²Ù… Ù¾Ú» ÙˆÙŠØªÙ†Ø§Ù… Û½ Ø³Ù†Ú¯Ø§Ù¾ÙˆØ± Û¾ ÙˆÚÙŠ Ù¾ÙŠÙ…Ø§Ù†ÙŠ ØªÙŠ Ø¹Ù…Ù„ ÚªÙŠÙˆ ÙˆÚƒÙŠ Ù¿Ùˆ. ÙÙ„Ù¾Ø§Ø¦Ù†ØŒ Ø§ÙˆÚ€Ø± Ø§Ù†ÚŠÙˆÙ†ÙŠØ´ÙŠØ§ØŒ Ø§ÙˆÚ€Ø± Ù…Ù„Ø§Ø¦ÙŠØ´ÙŠØ§ Û½ Ø§ÙˆÚ€Ø± ØªÙŠÙ…ÙˆØ± Û¾ Ø¹ÙŠØ³Ø§Ø¦ÙŠØª ØºØ§Ù„Ø¨ Ø¢Ù‡ÙŠ. &quot;ÙÙ„Ù¾Ø§Ø¦Ù†&quot; Ø§ÙŠØ´ÙŠØ§ Û¾ Ø³Ú€ Ú©Ø§Ù† ÙˆÚÙŠ Ø±ÙˆÙ…Ù† ÚªÙŠÙ¿ÙˆÙ„Úª Ø¢Ø¨Ø§Ø¯ÙŠ Ø¢Ù‡ÙŠ. Ù¾Ø±ØªÚ¯Ø§Ù„ÙŠ Ø­ÚªÙ…Ø±Ø§Ù†ÙŠ Ø¬ÙŠ ØªØ§Ø±ÙŠØ® Ø¬ÙŠ ÚªØ±ÙŠ Ø§ÙˆÚ€Ø± ØªÙŠÙ…ÙˆØ± Ù¾Ú» Ú¯Ù‡Ú»Ùˆ ÚªØ±ÙŠ Ø±ÙˆÙ…Ù† ÚªÙŠÙ¿ÙˆÙ„Úª Ø¢Ù‡ÙŠ.\n\nQuestion: Ø±ÙˆÙ…Ù† ÚªÙŠÙ¿ÙˆÙ„Úª Ø¬ÙŠ Ø¢Ø¨Ø§Ø¯ÙŠ ÚªÙ‡Ú™ÙŠ Ø§ÙŠØ´ÙŠØ§Ø¦ÙŠ Ù…Ù„Úª Û¾ ØºØ§Ù„Ø¨ Ø¢Ù‡ÙŠØŸ\nPredicted Answer: .\nActual Answer: ÙÙ„Ù¾Ø§Ø¦Ù†\nScore: 0.0000\n--------------------------------------------------------------------------------\nExample 3:\nContext: ÙŠÙŠÙ„ Ø¬ÙŠ Ù…ÙŠÙˆØ²ÙŠÙ… Ø¬Ø§ Ù…Ø¬Ù…ÙˆØ¹Ø§ Ù¾Ú» Ø¨ÙŠÙ† Ø§Ù„Ø§Ù‚ÙˆØ§Ù…ÙŠ Ù‚Ø¯ Ø¬Ø§ Ø¢Ù‡Ù†. ÙŠÙ„ÙŠ ÙŠÙˆÙ†ÙŠÙˆØ±Ø³Ù½ÙŠ Ø¢Ø±Ù½ Ú¯ÙŠÙ„Ø±ÙŠØŒ Ù…Ù„Úª Ø¬Ùˆ Ù¾Ù‡Ø±ÙŠÙˆÙ† ÙŠÙˆÙ†ÙŠÙˆØ±Ø³Ù½ÙŠ Ø³Ø§Ù† Ù„Ø§Ú³Ø§Ù¾ÙŠÙ„ Ø¢Ø±Ù½ Ù…ÙŠÙˆØ²ÙŠÙ…ØŒ 180,000 Ú©Ø§Ù† ÙˆÚŒÙŠÚª ÚªÙ…Ù† ØªÙŠ Ù…Ø´ØªÙ…Ù„ Ø¢Ù‡ÙŠØŒ Ø¬Ù†Ù‡Ù† Û¾ Ù¾Ø±Ø§Ú»ÙŠ Ù…Ø§Ø³Ù½Ø±Ø³ Û½ Ø¬Ø¯ÙŠØ¯ Ø¢Ø±Ù½ Ø¬Ø§ Ø§Ù‡Ù… Ù…Ø¬Ù…ÙˆØ¹Ø§ Ø´Ø§Ù…Ù„ Ø¢Ù‡Ù†ØŒ Ø³ÙˆØ§Ø±Ù½Ù½ Û½ ÚªØ§Ù‡Ù† Ø¹Ù…Ø§Ø±ØªÙ† Û¾. Ø¨Ø¹Ø¯ Û¾ØŒ &quot;Ù„ÙˆØ¦Ø³ ÚªØ§Ù†&quot; Ø¬Ùˆ Ù¾Ù‡Ø±ÙŠÙˆÙ† ÙˆÚÙŠ Ù¾ÙŠÙ…Ø§Ù†ÙŠ ØªÙŠ Ø¢Ù…Ø±ÙŠÚªÙŠ ÚªÙ… (1953)ØŒ Ø¨Ø­Ø§Ù„ ÚªÙŠÙˆ ÙˆÙŠÙˆ Û½ ÚŠØ³Ù…Ø¨Ø± 2006 Û¾ Ù»ÙŠÙ‡Ø± Ú©ÙˆÙ„ÙŠÙˆ ÙˆÙŠÙˆ. ÙŠÙ„ÙŠ Ø³ÙŠÙ†Ù½Ø± ÙØ§Ø± Ø¨Ø±Ù½Ø´ Ø¢Ø±Ù½ØŒ Ø¨Ø±Ø·Ø§Ù†ÙŠÙ‡ Ú©Ø§Ù† Ù»Ø§Ù‡Ø± Ø¨Ø±Ø·Ø§Ù†ÙˆÙŠ Ø¢Ø±Ù½ Ø¬Ùˆ Ø³Ú€ Ú©Ø§Ù† ÙˆÚÙˆ Ù…Ø¬Ù…ÙˆØ¹ÙˆØŒ Ù¾Ø§Ù„ Ø¬ÙŠ ØªØ­ÙÙŠ Ù…Ø§Ù† ÙˆÚŒØ§ÙŠÙˆ. Ù…ÙŠÙ„Ù† Û½ Ù»ÙŠ ÚªØ§Ù‡Ù† Ø¬ÙŠ ÙºÙ‡ÙŠÙ„ Ø¹Ù…Ø§Ø±Øª Û¾ Ø±Ú©ÙŠÙ„ Ø¢Ù‡ÙŠ.\n\nQuestion: ÙŠÙ„ÙŠ Ø³ÙŠÙ†Ù½Ø± ÙØ§Ø± Ø¨Ø±Ù½Ø´ Ø¢Ø±Ù½ Ø¬ÙŠ Ø¹Ù…Ø§Ø±Øª Ú©ÙŠ ÚªÙ†Ù‡Ù† Ù»ÙŠÙ‡Ø± ÚŠØ²Ø§Ø¦ÙŠÙ† ÚªÙŠÙˆØŸ\nPredicted Answer: .\nActual Answer: Ù„ÙˆØ¦Ø³ Ø®Ø§Ù†\nScore: 0.0000\n--------------------------------------------------------------------------------\nExample 4:\nContext: Ø¬ÙŠØ¦Ù† ØªÙ‡ 1954Ø¹ Ø¬ÙˆÙ† ÚªØ§Ù†Ú¯Ø±ÙŠØ³ÙŠ Ú†ÙˆÙ†ÚŠÙˆÙ† ÙˆÙŠØ¬Ù‡Ùˆ Ø¢ÙŠÙˆÙ†ØŒ Û½ Ø§Ù‡Ùˆ ÙˆØ§Ø¶Ø­ Ù¿ÙŠÙˆ ØªÙ‡ Ø±ÙŠÙ¾Ø¨Ù„ÚªÙ† Ù»Ù†Ù‡ÙŠ Ø§ÙŠÙˆØ§Ù†Ù† Û¾ Ù¾Ù†Ù‡Ù†Ø¬ÙŠ Ù¾ØªÙ„ÙŠ Ø§ÚªØ«Ø±ÙŠØª ÙˆÚƒØ§Ø¦Ú» Ø¬Ùˆ Ø®Ø·Ø±Ùˆ Ø¢Ù‡ÙŠØŒ Ø¢Ø¦Ø²Ù† Ù‡Ø§ÙˆØ± Ø§Ù†Ù‡Ù† Ù…Ø§Ú»Ù‡Ù† Û¾ Ø´Ø§Ù…Ù„ Ù‡ÙˆØŒ Ø¬Ù† Ù†Ù‚ØµØ§Ù† Ø¬Ùˆ Ø§Ù„Ø²Ø§Ù… Ø§ÙˆÙ„ÚŠ Ú¯Ø§Ø±ÚŠ ØªÙŠ Ù„Ú³Ø§ÙŠÙˆØŒ Û½ Ø³Ø§Ú„ÙŠ ÚŒØ± Ø·Ø±ÙØ§Ù† Ø´ÚªÙŠ ÚªÙˆØ´Ø´Ù† Ú©ÙŠ Ø±ÙˆÚªÚ» Ø¬ÙŠ Ø°Ù…ÙŠÙˆØ§Ø±ÙŠ ÙˆØ±ØªÙŠ. ÙˆÙ†Ú¯ GOP Ø¬ÙŠ ÚªÙ†Ù½Ø±ÙˆÙ„ ÙˆÙºÚ» Ù„Ø§Ø¡. Ø¢Ø¦Ø²Ù† Ù‡Ø§ÙˆØ± ÙˆØ±ÙŠ Ù‡Úª Ø§Ø¹ØªØ¯Ø§Ù„ Ù¾Ø³Ù†Ø¯ØŒ ØªØ±Ù‚ÙŠ Ù¾Ø³Ù†Ø¯ Ø±ÙŠÙ¾Ø¨Ù„ÚªÙ† Ø¬ÙŠ Ø­ÙŠØ«ÙŠØª Ø³Ø§Ù† Ù¾Ù†Ù‡Ù†Ø¬ÙŠ Ù¾ÙˆØ²ÙŠØ´Ù† Ú©ÙŠ Ø¨ÙŠØ§Ù† ÚªÙŠÙˆ: Ù…Ù†Ù‡Ù†Ø¬Ùˆ ØµØ±Ù Ù‡Úª Ù…Ù‚ØµØ¯ Ø¢Ù‡ÙŠ ... Û½ Ø§Ù‡Ùˆ Ø¢Ù‡ÙŠ ØªÙ‡ Ù‡Ù† Ù…Ù„Úª Û¾ Ù‡Úª Ù…Ø¶Ø¨ÙˆØ· ØªØ±Ù‚ÙŠ Ù¾Ø³Ù†Ø¯ Ø±ÙŠÙ¾Ø¨Ù„ÚªÙ† Ù¾Ø§Ø±Ù½ÙŠ ÙºØ§Ù‡ÙŠ. Ø¬ÙŠÚªÚÙ‡Ù† â€Ø³Ø§Ú„ÙŠâ€œ ÙˆÙ†Ú¯ ÙˆÚ™Ù‡Ú» Ú†Ø§Ù‡ÙŠ Ù¿ÙŠØŒ ØªÙ‡ Ø§Ù‡ÙŠ Ø­Ø§ØµÙ„ ÚªØ±Ú» ÙˆØ§Ø±Ø§ Ø¢Ù‡Ù†... Ù…Ù†Ù‡Ù†Ø¬ÙŠ Ø®ØªÙ… Ù¿ÙŠÚ» Ú©Ø§Ù† Ø§Ú³ØŒ ÙŠØ§ ØªÙ‡ Ø§Ù‡Ø§ Ø±ÙŠÙ¾Ø¨Ù„ÚªÙ† Ù¾Ø§Ø±Ù½ÙŠ ØªØ±Ù‚ÙŠ Ù¾Ø³Ù†Ø¯ÙŠØ¡Ù Ø¬ÙŠ Ø¹ÚªØ§Ø³ÙŠ ÚªÙ†Ø¯ÙŠ ÙŠØ§ Ù…Ø§Ù† Ù‡Ø§Ú»ÙŠ Ø³Ø§Ú»Ù† Ú¯Ú Ù†Ù‡ ÙˆÙŠÙ†Ø¯Ø³.\n\nQuestion: GOP Ø¬ÙŠ ÚªÙ‡Ú™ÙŠ ÙˆÙ†Ú¯ Ø¢Ø¦Ø²Ù† Ù‡Ø§ÙˆØ± Ø¬ÙŠ Ù…Ø®Ø§Ù„ÙØª ÚªØ¦ÙŠ Ù‡Ø¦ÙŠØŸ\nPredicted Answer: .\nActual Answer: Ø³Ø§Ú„Ùˆ\nScore: 0.0000\n--------------------------------------------------------------------------------\nExample 5:\nContext: Ø§Ù¾Ø±ÙŠÙ„ 2013 Û¾ØŒ Ù…Ø§Ø±ÙˆÙ„ Û½ Ù»ÙŠÙ† ÚŠØ²Ù†ÙŠ Ø¬Ù…Ø§Ø¹ØªÙŠ Ø§Ø¬Ø²Ø§Ø¡ Ú¯ÚÙŠÙ„ Ù…Ù†ØµÙˆØ¨Ù† Ø¬Ùˆ Ø§Ø¹Ù„Ø§Ù† ÚªØ±Ú» Ø´Ø±ÙˆØ¹ ÚªÙŠÙˆ. ABC Ø³Ø§Ù†ØŒ Ù‡Úª ÙˆÙ†Ø³ Ø§Ù¾ÙˆÙ† Ø§ÙŠ Ù½Ø§Ø¦ÙŠÙ… Ú¯Ø±Ø§ÙÚª Ù†Ø§ÙˆÙ„ Ø³ÙŠÙ¾Ù½Ù…Ø¨Ø± Û¾ Ø§Ø´Ø§Ø¹Øª Ù„Ø§Ø¡Ù Ø§Ø¹Ù„Ø§Ù† ÚªÙŠÙˆ ÙˆÙŠÙˆ. ÚŠØ²Ù†ÙŠ Ø³Ø§Ù† Ú¯ÚØŒ Ù…Ø§Ø±ÙˆÙ„ Ø¢ÚªÙ½ÙˆØ¨Ø± 2013 Û¾ Ø§Ø¹Ù„Ø§Ù† ÚªÙŠÙˆ ØªÙ‡ Ø¬Ù†ÙˆØ±ÙŠ 2014 Û¾ Ø§Ú¾Ùˆ Ù¾Ù†Ú¾Ù†Ø¬Ùˆ Ù¾Ú¾Ø±ÙŠÙˆÙ† Ø¹Ù†ÙˆØ§Ù† Ù¾Ù†Ú¾Ù†Ø¬ÙŠ Ú¯ÚÙŠÙ„ ÚŠØ²Ù†ÙŠ ÚªÙ†Ú¯ÚŠÙ…Ø² Ø§Ù…Ù¾Ø±Ù†Ù½ Ø³ÙŠÚªØ±Ø² Ø¢Ù Ø¯ÙŠ ÙˆÙŠØ¦Ø±ÚŠ ØªØ­Øª Ø¬Ø§Ø±ÙŠ ÚªÙ†Ø¯ÙˆØŒ Ú¾Úª â€Ù¾Ù†Ø¬â€œ-Ù…Ø³Ø¦Ù„Ùˆ Ù…Ù†Ø³Ù½Ø±. 3 Ø¬Ù†ÙˆØ±ÙŠ 2014 ØªÙŠØŒ Ø³Ø§Ù¿ÙŠ ÚŠØ²Ù†ÙŠ Ø¬ÙŠ Ù…Ø§ØªØ­Øª ÚªÙ…Ù¾Ù†ÙŠ Ù„ÙˆÚªØ§Ø³ÙÙŠÙ„Ù… Ù„Ù…ÙŠÙ½ÙŠÚŠØŒ LLC Ø§Ø¹Ù„Ø§Ù† ÚªÙŠÙˆ ØªÙ‡ 2015 ØªØ§Ø¦ÙŠÙ†ØŒ Ø§Ø³Ù½Ø§Ø± ÙˆØ§Ø± Ù…Ø²Ø§Ø­ÙŠÙ‡ Ù‡Úª Ú€ÙŠØ±Ùˆ Ù»ÙŠÙ‡Ø± Ù…Ø§Ø±ÙˆÙ„ Ù¾Ø§Ø±Ø§Ù† Ø´Ø§ÙŠØ¹ ÚªÙŠÙˆ ÙˆÙŠÙ†Ø¯Ùˆ.\n\nQuestion: ÙˆÙ†Ø³ Ø§Ù¾ÙˆÙ† Ø§ÙŠ Ù½Ø§Ø¦ÙŠÙ… Ù†Ø§ÙˆÙ„ ÚªÙŠØªØ±Ø§ Ù…Ø³Ø¦Ù„Ø§ Ù‡Ø¦Ø§ØŸ\nPredicted Answer: .\nActual Answer: Ù¾Ù†Ø¬\nScore: 0.0000\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":22}]}