{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10230337,"sourceType":"datasetVersion","datasetId":6325329}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentence-transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:14:43.412588Z","iopub.execute_input":"2024-12-17T22:14:43.412850Z","iopub.status.idle":"2024-12-17T22:14:52.903537Z","shell.execute_reply.started":"2024-12-17T22:14:43.412825Z","shell.execute_reply":"2024-12-17T22:14:52.902424Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.26.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments\nimport torch\nimport os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForQuestionAnswering, \n    TrainingArguments, \n    Trainer\n)\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:14:52.905976Z","iopub.execute_input":"2024-12-17T22:14:52.906361Z","iopub.status.idle":"2024-12-17T22:14:52.982714Z","shell.execute_reply.started":"2024-12-17T22:14:52.906331Z","shell.execute_reply":"2024-12-17T22:14:52.981939Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def load_and_split_data(\n    input_csv, \n    train_ratio=0.7, \n    test_ratio=0.15, \n    random_seed=42\n):\n   \n    df = pd.read_csv(input_csv)\n    df = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n    total_samples = len(df)\n    train_end = int(total_samples * train_ratio)\n    val_end = train_end + int(total_samples * test_ratio)\n    \n    train_df = df.iloc[:train_end]\n    val_df = df.iloc[train_end:val_end]\n    test_df = df.iloc[val_end:]\n    \n    def transform_subset(subset_df):\n        records = []\n        for _, row in tqdm(subset_df.iterrows(), total=len(subset_df), \n                            desc=\"Transforming data\"):\n            answer_start = row['context'].find(row['answer']) \\\n                if not row['is_impossible'] else -1\n            \n            record = {\n                \"id\": row['id'],\n                \"title\": row['title'],\n                \"context\": row['context'],\n                \"question\": row['question'],\n                \"answers\": {\n                    \"text\": [row['answer']] if not row['is_impossible'] else [],\n                    \"answer_start\": [answer_start] if not row['is_impossible'] else []\n                },\n                \"is_impossible\": row['is_impossible'],\n                \"original_answer\": row['answer']\n            }\n            records.append(record)\n        return records\n    \n    train_records = transform_subset(train_df)\n    val_records = transform_subset(val_df)\n    test_records = transform_subset(test_df)\n    \n    train_dataset = Dataset.from_pandas(pd.DataFrame(train_records))\n    val_dataset = Dataset.from_pandas(pd.DataFrame(val_records))\n    test_dataset = Dataset.from_pandas(pd.DataFrame(test_records))\n    \n    return DatasetDict({\n        \"train\": train_dataset, \n        \"validation\": val_dataset, \n        \"test\": test_dataset\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:14:52.983902Z","iopub.execute_input":"2024-12-17T22:14:52.984462Z","iopub.status.idle":"2024-12-17T22:14:52.992536Z","shell.execute_reply.started":"2024-12-17T22:14:52.984422Z","shell.execute_reply":"2024-12-17T22:14:52.991717Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ndef preprocess_function(tokenizer, examples):\n    \n    tokenized = tokenizer(\n        examples[\"question\"], \n        examples[\"context\"], \n        truncation=True, \n        padding=\"max_length\", \n        max_length=512\n    )\n    \n    start_positions = []\n    end_positions = []\n    for answers in examples[\"answers\"]:\n        if answers[\"text\"]:\n            start = answers[\"answer_start\"][0]\n            end = start + len(answers[\"text\"][0])\n        else:\n            start = 0\n            end = 0\n        start_positions.append(start)\n        end_positions.append(end)\n    \n    tokenized[\"start_positions\"] = start_positions\n    tokenized[\"end_positions\"] = end_positions\n    \n    return tokenized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:14:52.993649Z","iopub.execute_input":"2024-12-17T22:14:52.993944Z","iopub.status.idle":"2024-12-17T22:14:53.006053Z","shell.execute_reply.started":"2024-12-17T22:14:52.993919Z","shell.execute_reply":"2024-12-17T22:14:53.005369Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport shutil\nimport os\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments, Trainer\n\ndef train_qa_model(\n    data, \n    model_name=\"roberta-base\", \n    num_epochs=1,\n    learning_rate=2e-5,\n    batch_size=32\n):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n    processed_data = data.map(\n        lambda x: preprocess_function(tokenizer, x), \n        batched=True, \n        remove_columns=data[\"train\"].column_names\n    )\n    \n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        evaluation_strategy=\"epoch\",\n        learning_rate=learning_rate,\n        per_device_train_batch_size=batch_size,  \n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=num_epochs,\n        weight_decay=0.01,\n        logging_dir=\"./logs\",\n        logging_steps=10,\n        save_steps=500,\n        save_total_limit=2,\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=processed_data[\"train\"],\n        eval_dataset=processed_data[\"validation\"],\n        tokenizer=tokenizer,\n    )\n    \n    trainer.train()\n    model.save_pretrained(\"./fine_tuned_model\")\n    tokenizer.save_pretrained(\"./fine_tuned_model\")\n    zip_path = \"/kaggle/working/fine_tuned_model.zip\"\n    shutil.make_archive(\"/kaggle/working/fine_tuned_model\", 'zip', \"./fine_tuned_model\")\n    print(f\"Model saved and zipped to {zip_path}\")\n    return trainer, tokenizer, model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:47:12.619191Z","iopub.execute_input":"2024-12-17T22:47:12.620000Z","iopub.status.idle":"2024-12-17T22:47:12.627533Z","shell.execute_reply.started":"2024-12-17T22:47:12.619967Z","shell.execute_reply":"2024-12-17T22:47:12.626722Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"input_csv = \"/kaggle/input/sindhi/SQuAD_Translated_Sindhi.csv\"\ndata = load_and_split_data(input_csv)\ntrainer, tokenizer, model = train_qa_model(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:47:16.583125Z","iopub.execute_input":"2024-12-17T22:47:16.583483Z","iopub.status.idle":"2024-12-17T22:51:24.105064Z","shell.execute_reply.started":"2024-12-17T22:47:16.583451Z","shell.execute_reply":"2024-12-17T22:51:24.104161Z"}},"outputs":[{"name":"stderr","text":"Transforming data: 100%|██████████| 3500/3500 [00:00<00:00, 15580.95it/s]\nTransforming data: 100%|██████████| 750/750 [00:00<00:00, 15055.87it/s]\nTransforming data: 100%|██████████| 750/750 [00:00<00:00, 15144.15it/s]\nSome weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d95e9a4e95c24d5188d0614e39282907"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/750 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20fbfd4421ed4aa5aa605d32eb80af47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/750 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd6cbcf5b84b4dad9fd72089f7807bef"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/tmp/ipykernel_23/1236429608.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [110/110 03:19, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.063600</td>\n      <td>3.170593</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Model saved and zipped to /kaggle/working/fine_tuned_model.zip\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef evaluate_qa_model(\n    model, \n    tokenizer, \n    test_dataset, \n    device=None\n):\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    model = model.to(device)\n    model.eval()\n    \n    exact_match_scores = []\n    cosine_sim_scores = []\n    sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n    \n    with torch.no_grad():\n        for example in tqdm(test_dataset, desc=\"Evaluating\"):\n            inputs = tokenizer(\n                example['question'], \n                example['context'], \n                return_tensors='pt', \n                max_length=512, \n                truncation=True\n            ).to(device)\n            \n            outputs = model(**inputs)\n            start_logits = outputs.start_logits\n            end_logits = outputs.end_logits\n            \n            start_idx = torch.argmax(start_logits).item()\n            end_idx = torch.argmax(end_logits).item()\n            input_ids = inputs['input_ids'][0]\n            predicted_answer_tokens = input_ids[start_idx:end_idx+1]\n            predicted_answer = tokenizer.decode(predicted_answer_tokens).strip()\n            \n            ground_truth = example['original_answer']\n            exact_match_scores.append(int(predicted_answer.lower() == ground_truth.lower()))\n            \n            pred_emb = sentence_model.encode([predicted_answer])\n            gt_emb = sentence_model.encode([ground_truth])\n            cosine_sim_scores.append(cosine_similarity(pred_emb, gt_emb)[0][0])\n    \n    metrics = {\n        \"exact_match_rate\": np.mean(exact_match_scores),\n        \"avg_cosine_similarity\": np.mean(cosine_sim_scores),\n        \"cosine_sim_std\": np.std(cosine_sim_scores)\n    }\n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:43:52.067476Z","iopub.status.idle":"2024-12-17T22:43:52.067795Z","shell.execute_reply.started":"2024-12-17T22:43:52.067614Z","shell.execute_reply":"2024-12-17T22:43:52.067627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_metrics = evaluate_qa_model(model, tokenizer, data[\"test\"])\nprint(\"Evaluation Metrics:\")\nfor key, value in test_metrics.items():\n    print(f\"{key}: {value}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:52:33.122972Z","iopub.execute_input":"2024-12-17T22:52:33.123316Z","iopub.status.idle":"2024-12-17T22:52:33.129443Z","shell.execute_reply.started":"2024-12-17T22:52:33.123286Z","shell.execute_reply":"2024-12-17T22:52:33.128568Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers', 'is_impossible', 'original_answer'],\n        num_rows: 3500\n    })\n    validation: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers', 'is_impossible', 'original_answer'],\n        num_rows: 750\n    })\n    test: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers', 'is_impossible', 'original_answer'],\n        num_rows: 750\n    })\n})\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from transformers import pipeline\nimport numpy as np\n\ndef display_results_qa_model( tokenizer, model, dataset, num_examples=5):\n    \"\"\"\n    Evaluate the fine-tuned model on a few examples.\n    \"\"\"\n    # Load fine-tuned model into a QA pipeline\n    qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n\n    # Select the first `num_examples` from the validation dataset\n    examples = dataset[\"validation\"].select(range(num_examples))\n    \n    print(\"Evaluating on 5 examples...\\n\")\n    \n    for i, example in enumerate(examples):\n        context = example['context']\n        question = example['question']\n        actual_answer = example['original_answer']  # First answer in the list\n        \n        # Get the model's prediction\n        prediction = qa_pipeline(question=question, context=context)\n        predicted_answer = prediction['answer']\n        \n        # Print the details\n        print(f\"Example {i+1}:\")\n        print(f\"Context: {context}\\n\")\n        print(f\"Question: {question}\")\n        print(f\"Predicted Answer: {predicted_answer}\")\n        print(f\"Actual Answer: {actual_answer}\")\n        print(f\"Score: {prediction['score']:.4f}\")\n        print(\"-\" * 80)\n\n# Example usage\n# trainer, tokenizer, model = train_qa_model(data)\ndisplay_results_qa_model( tokenizer, model, data, num_examples=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:53:14.000024Z","iopub.execute_input":"2024-12-17T22:53:14.000392Z","iopub.status.idle":"2024-12-17T22:53:14.295367Z","shell.execute_reply.started":"2024-12-17T22:53:14.000355Z","shell.execute_reply":"2024-12-17T22:53:14.294468Z"}},"outputs":[{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on 5 examples...\n\nExample 1:\nContext: 1973ع جي چونڊن کان پوءِ سوزلينڊ جو آئين بادشاهه سوڀوزا II معطل ڪيو، جنهن بعد ۾ 1982ع ۾ پنهنجي وفات تائين فرمان ذريعي ملڪ تي حڪومت ڪئي. ان وقت سوڀوزا II 61 سالن تائين سوزيلينڊ تي حڪومت ڪئي، جنهن کيس تاريخ جو سڀ کان ڊگهو حڪمران بڻائي ڇڏيو. هن جي موت جي پٺيان هڪ ريجنسي آئي، جنهن ۾ ”راڻي ريجنٽ ڊيزيلي شونگوي“ 1984ع تائين رياست جي سربراهه رهي، جڏهن هوءَ ليڪوڪو طرفان هٽائي وئي ۽ ان جي جاءِ تي راڻي ماءُ نٽفومبي ٽفوالا مقرر ڪئي وئي. Mswati III، Ntfombi جو پٽ، 25 اپريل 1986 تي سوازيلينڊ جي بادشاهه ۽ انگونياما جي حيثيت سان تاج ڪيو ويو.\n\nQuestion: سوزيلينڊ ۾ 61 سالن تائين ڪهڙي رينٽ حڪومت ڪئي؟\nPredicted Answer: .\nActual Answer: راڻي ريجنٽ ڊيزيلي شونگوي\nScore: 0.0000\n--------------------------------------------------------------------------------\nExample 2:\nContext: اسلام ڏکڻ اوڀر ايشيا ۾ سڀ کان وڏي پيماني تي استعمال ٿيندڙ مذهب آهي، جنهن ۾ تقريبن 240 ملين پيروڪار آهن جيڪي سڄي آبادي جو 40 سيڪڙو تائين ترجمو ڪن ٿا، انڊونيشيا، برونائي، ملائيشيا ۽ ڏاکڻي فلپائن ۾ اڪثريت انڊونيشيا سان گڏ انڊونيشيا جي چوڌاري سڀ کان وڏو ۽ سڀ کان وڌيڪ آبادي وارو مسلمان ملڪ آهي. دنيا. ڏکڻ اوڀر ايشيا جا ملڪ ڪيترن ئي مختلف مذهبن تي عمل ڪن ٿا. ٿائلينڊ، ڪمبوڊيا، لاوس، برما، ويٽنام ۽ سنگاپور ۾ ٻڌمت غالب آهي. ابن ڏاڏن جي پوڄا ۽ ڪنفيوشسزم پڻ ويتنام ۽ سنگاپور ۾ وڏي پيماني تي عمل ڪيو وڃي ٿو. فلپائن، اوڀر انڊونيشيا، اوڀر ملائيشيا ۽ اوڀر تيمور ۾ عيسائيت غالب آهي. &quot;فلپائن&quot; ايشيا ۾ سڀ کان وڏي رومن ڪيٿولڪ آبادي آهي. پرتگالي حڪمراني جي تاريخ جي ڪري اوڀر تيمور پڻ گهڻو ڪري رومن ڪيٿولڪ آهي.\n\nQuestion: رومن ڪيٿولڪ جي آبادي ڪهڙي ايشيائي ملڪ ۾ غالب آهي؟\nPredicted Answer: .\nActual Answer: فلپائن\nScore: 0.0000\n--------------------------------------------------------------------------------\nExample 3:\nContext: ييل جي ميوزيم جا مجموعا پڻ بين الاقوامي قد جا آهن. يلي يونيورسٽي آرٽ گيلري، ملڪ جو پهريون يونيورسٽي سان لاڳاپيل آرٽ ميوزيم، 180,000 کان وڌيڪ ڪمن تي مشتمل آهي، جنهن ۾ پراڻي ماسٽرس ۽ جديد آرٽ جا اهم مجموعا شامل آهن، سوارٽٽ ۽ ڪاهن عمارتن ۾. بعد ۾، &quot;لوئس ڪان&quot; جو پهريون وڏي پيماني تي آمريڪي ڪم (1953)، بحال ڪيو ويو ۽ ڊسمبر 2006 ۾ ٻيهر کوليو ويو. يلي سينٽر فار برٽش آرٽ، برطانيه کان ٻاهر برطانوي آرٽ جو سڀ کان وڏو مجموعو، پال جي تحفي مان وڌايو. ميلن ۽ ٻي ڪاهن جي ٺهيل عمارت ۾ رکيل آهي.\n\nQuestion: يلي سينٽر فار برٽش آرٽ جي عمارت کي ڪنهن ٻيهر ڊزائين ڪيو؟\nPredicted Answer: .\nActual Answer: لوئس خان\nScore: 0.0000\n--------------------------------------------------------------------------------\nExample 4:\nContext: جيئن ته 1954ع جون ڪانگريسي چونڊون ويجهو آيون، ۽ اهو واضح ٿيو ته ريپبلڪن ٻنهي ايوانن ۾ پنهنجي پتلي اڪثريت وڃائڻ جو خطرو آهي، آئزن هاور انهن ماڻهن ۾ شامل هو، جن نقصان جو الزام اولڊ گارڊ تي لڳايو، ۽ ساڄي ڌر طرفان شڪي ڪوششن کي روڪڻ جي ذميواري ورتي. ونگ GOP جي ڪنٽرول وٺڻ لاء. آئزن هاور وري هڪ اعتدال پسند، ترقي پسند ريپبلڪن جي حيثيت سان پنهنجي پوزيشن کي بيان ڪيو: منهنجو صرف هڪ مقصد آهي ... ۽ اهو آهي ته هن ملڪ ۾ هڪ مضبوط ترقي پسند ريپبلڪن پارٽي ٺاهي. جيڪڏهن ”ساڄي“ ونگ وڙهڻ چاهي ٿي، ته اهي حاصل ڪرڻ وارا آهن... منهنجي ختم ٿيڻ کان اڳ، يا ته اها ريپبلڪن پارٽي ترقي پسنديءَ جي عڪاسي ڪندي يا مان هاڻي ساڻن گڏ نه ويندس.\n\nQuestion: GOP جي ڪهڙي ونگ آئزن هاور جي مخالفت ڪئي هئي؟\nPredicted Answer: .\nActual Answer: ساڄو\nScore: 0.0000\n--------------------------------------------------------------------------------\nExample 5:\nContext: اپريل 2013 ۾، مارول ۽ ٻين ڊزني جماعتي اجزاء گڏيل منصوبن جو اعلان ڪرڻ شروع ڪيو. ABC سان، هڪ ونس اپون اي ٽائيم گرافڪ ناول سيپٽمبر ۾ اشاعت لاءِ اعلان ڪيو ويو. ڊزني سان گڏ، مارول آڪٽوبر 2013 ۾ اعلان ڪيو ته جنوري 2014 ۾ اھو پنھنجو پھريون عنوان پنھنجي گڏيل ڊزني ڪنگڊمز امپرنٽ سيڪرز آف دي ويئرڊ تحت جاري ڪندو، ھڪ ”پنج“-مسئلو منسٽر. 3 جنوري 2014 تي، ساٿي ڊزني جي ماتحت ڪمپني لوڪاسفيلم لميٽيڊ، LLC اعلان ڪيو ته 2015 تائين، اسٽار وار مزاحيه هڪ ڀيرو ٻيهر مارول پاران شايع ڪيو ويندو.\n\nQuestion: ونس اپون اي ٽائيم ناول ڪيترا مسئلا هئا؟\nPredicted Answer: .\nActual Answer: پنج\nScore: 0.0000\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":22}]}